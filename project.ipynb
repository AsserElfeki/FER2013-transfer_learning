{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6176ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e08eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe04620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/archive/train'\n",
    "test_folder_path = './data/archive/test'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1295da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors_list = []\n",
    "test_tensors_list = []\n",
    "\n",
    "train_pixels_list = []\n",
    "test_pixels_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0008a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'angry': 0, \n",
    "             'disgust': 1, \n",
    "             'fear': 2, \n",
    "             'happy': 3, \n",
    "             'sad': 4, \n",
    "             'surprise': 5, \n",
    "             'neutral': 6}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e90e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images in folder \"happy, 3\"...\n",
      "Processing images in folder \"sad, 4\"...\n",
      "Processing images in folder \"fear, 2\"...\n",
      "Processing images in folder \"surprise, 5\"...\n",
      "Processing images in folder \"neutral, 6\"...\n",
      "Processing images in folder \"angry, 0\"...\n",
      "Processing images in folder \"disgust, 1\"...\n",
      "(3, '\"97 61 32 22 18 24 33 44 50 52 56 62 61 61 69 71 74 77 79 81 83 82 80 82 83 82 84 84 84 87 84 87 88 85 84 86 89 90 92 90 98 106 129 202 218 190 196 22 33 42 57 65 68 71 67 65 63 62 63 66 69 73 75 74 73 73 75 79 82 83 84 83 83 88 90 90 92 89 86 87 90 92 97 99 104 109 107 116 124 143 194 226 192 182 91 92 75 73 76 75 70 70 63 61 64 67 70 71 69 71 73 75 77 79 80 80 80 82 83 85 90 93 95 96 92 91 90 94 97 105 105 111 118 120 133 139 152 196 238 200 177 95 91 73 74 78 76 72 64 65 62 65 73 71 67 73 74 75 76 79 81 82 81 80 82 87 90 93 94 96 97 90 95 96 99 97 102 101 104 104 108 135 145 154 201 238 214 217 94 91 74 77 81 76 71 70 67 67 68 67 67 70 73 75 71 69 71 77 82 85 86 84 90 93 94 93 94 94 86 93 97 93 79 75 78 84 87 102 126 143 158 208 244 224 212 92 91 71 72 78 77 75 69 65 72 76 67 71 76 67 58 56 56 60 64 69 73 76 84 89 92 94 93 94 95 88 89 87 77 67 63 70 83 96 113 121 134 153 204 243 205 138 89 89 72 74 77 76 77 68 73 79 76 70 73 75 68 64 64 62 60 58 61 69 78 84 87 90 95 97 98 100 96 80 74 74 90 98 101 102 114 133 143 147 155 198 236 200 127 93 85 70 76 76 76 74 71 72 76 70 77 78 88 92 93 90 79 73 63 48 40 55 72 87 101 106 111 114 110 81 65 72 93 99 110 121 128 129 136 150 169 164 192 229 206 136 82 89 81 75 73 77 77 78 75 75 83 83 83 89 80 69 60 64 70 75 65 32 36 54 73 100 116 126 134 107 44 57 67 59 46 45 50 79 101 131 149 166 193 200 214 225 160 81 93 79 74 80 82 79 66 82 78 81 79 70 54 51 67 79 59 48 49 51 46 37 45 66 88 109 134 149 115 58 57 41 40 43 47 44 46 55 70 101 159 205 204 205 230 184 78 94 89 75 79 86 76 77 80 85 77 56 45 28 39 66 117 89 65 59 57 68 54 46 66 84 100 123 144 131 101 83 64 79 99 113 119 116 124 91 48 101 197 218 215 212 178 88 94 89 87 85 86 80 79 80 79 60 53 65 69 82 89 105 109 98 73 75 81 60 61 61 73 94 122 152 144 102 94 80 65 74 103 131 148 161 157 99 89 177 219 204 176 173 83 95 94 87 89 96 79 78 84 68 56 75 94 105 106 114 99 86 74 65 69 62 60 67 64 74 94 120 144 152 123 107 92 77 65 68 93 121 151 167 143 137 178 213 185 166 179 82 92 97 96 88 87 82 78 67 61 72 84 91 105 98 86 77 65 66 72 82 79 72 69 65 69 87 119 143 171 155 137 111 105 86 66 74 82 109 149 179 202 188 207 186 171 181 81 95 99 95 97 95 82 76 78 74 66 71 81 83 80 77 79 81 86 91 98 85 70 63 61 73 87 112 146 190 164 150 137 123 118 115 117 113 124 139 151 172 185 186 179 175 183 109 109 102 99 102 95 89 91 92 95 96 100 89 84 88 88 96 99 99 94 85 76 67 70 68 68 78 100 152 189 163 157 152 124 132 143 143 135 157 164 187 185 175 188 182 182 178 107 109 105 105 99 94 95 90 97 99 100 108 111 112 115 115 111 104 102 94 77 70 70 70 72 75 78 103 157 198 172 161 156 144 138 147 153 132 145 178 182 197 194 177 184 189 179 104 106 98 101 98 94 90 102 103 103 111 121 126 125 127 121 118 113 112 99 74 68 76 79 79 83 85 107 150 195 184 172 164 162 157 153 164 157 134 154 160 177 191 176 188 189 178 102 98 108 95 98 90 97 102 102 101 111 118 124 125 130 127 128 124 121 107 84 76 81 83 84 86 92 109 147 190 192 178 168 151 162 167 160 161 160 154 164 165 181 183 186 187 178 99 110 99 96 89 102 100 93 101 101 101 100 113 124 134 138 131 116 107 96 80 74 75 79 87 89 98 115 156 193 200 187 175 143 144 157 155 145 151 147 149 157 169 181 183 187 174 107 105 93 84 101 103 96 93 96 97 98 97 107 120 132 131 115 97 89 80 71 73 77 80 92 98 114 125 157 188 205 204 172 115 116 141 150 148 144 144 137 155 159 179 186 175 162 104 92 86 94 101 94 94 90 85 88 95 101 103 112 119 112 100 90 88 81 78 84 87 83 92 103 122 132 151 174 196 208 198 126 83 100 125 140 137 136 133 145 146 169 184 161 156 103 86 77 96 97 85 89 85 82 86 87 92 93 99 99 98 88 84 86 83 83 86 80 81 88 98 114 134 152 168 179 192 195 167 92 62 87 116 125 123 132 141 148 158 171 146 139 95 81 85 92 84 83 83 80 83 82 83 81 87 87 87 84 76 87 91 74 72 81 77 78 77 89 100 118 138 147 166 175 177 171 122 76 52 86 102 112 121 134 144 136 157 134 136 93 82 92 88 82 75 75 79 77 78 80 75 81 83 79 68 78 89 91 79 65 66 77 76 69 78 85 100 118 125 139 155 174 160 129 94 49 54 82 104 118 133 144 131 160 135 136 88 85 98 86 80 72 71 69 73 74 76 73 76 68 58 76 87 86 82 81 62 35 24 29 45 65 69 77 85 88 103 151 179 162 139 115 67 40 62 87 114 130 146 132 156 136 136 82 86 95 87 77 77 73 68 76 71 68 70 64 53 58 77 83 84 82 84 81 63 43 33 27 28 44 68 69 70 100 162 176 165 142 112 76 41 46 73 116 135 150 145 147 135 136 79 80 91 85 77 79 77 72 76 71 72 66 54 51 70 74 78 85 86 82 82 79 71 57 43 29 27 42 65 97 136 159 170 172 160 124 86 60 45 64 121 151 150 160 144 132 136 82 72 89 84 79 79 79 76 74 75 74 61 62 66 69 73 82 85 84 86 86 81 75 70 48 45 50 51 67 96 120 146 164 165 160 128 95 81 47 60 119 161 146 165 145 130 136 88 69 87 87 80 81 83 84 81 81 74 59 69 76 66 68 82 84 78 81 85 83 81 76 65 64 64 68 91 116 125 142 151 150 141 114 100 94 64 67 122 157 148 165 140 132 136 90 69 82 91 79 85 87 83 85 89 86 70 68 69 65 63 71 79 83 82 80 78 76 77 76 75 79 91 101 107 115 118 116 134 135 108 104 97 83 82 131 149 157 166 131 134 134 95 76 69 90 87 88 85 87 84 89 95 76 73 76 63 60 63 59 57 48 57 51 52 56 58 52 58 62 63 73 76 76 70 70 100 86 90 103 100 99 140 140 152 156 127 136 134 95 83 71 84 92 87 85 83 86 88 100 93 79 77 55 51 42 25 54 64 63 95 77 92 108 100 90 122 128 129 162 147 150 44 67 87 96 113 106 107 141 140 155 152 132 140 131 94 90 73 78 95 88 87 80 81 85 98 105 92 85 67 45 18 40 95 126 107 152 141 148 170 165 163 201 204 205 217 200 180 52 95 103 102 118 116 116 139 141 163 141 133 136 131 90 90 72 71 88 88 90 82 73 82 94 107 106 94 87 70 8 13 57 98 46 90 66 102 127 120 120 146 132 114 105 63 50 78 132 111 110 131 120 130 136 145 170 136 137 133 135 84 84 70 69 75 85 88 87 72 77 96 108 112 99 90 93 65 2 0 6 12 8 7 5 8 14 15 25 31 16 0 0 49 146 121 113 110 129 132 138 129 148 160 136 139 132 133 82 80 72 71 68 82 85 91 80 69 91 105 110 104 86 85 96 69 17 11 7 25 23 24 30 30 35 54 70 49 11 44 140 135 127 113 115 133 147 132 127 152 141 134 134 132 130 83 77 70 69 67 77 81 91 86 68 79 99 108 109 92 90 79 92 88 40 40 37 43 42 39 34 51 76 92 93 60 134 142 136 131 117 126 146 155 128 136 151 134 133 131 134 134 83 73 66 62 66 71 76 86 88 74 71 95 108 110 100 95 92 80 92 104 89 92 99 97 98 99 115 149 135 126 151 159 136 145 136 125 138 153 152 127 143 139 133 132 132 130 134 88 74 66 55 59 75 75 80 88 93 68 82 108 116 110 102 96 96 85 88 101 108 104 112 116 132 141 138 146 164 172 146 150 141 138 148 155 168 145 139 144 129 135 130 130 131 130 89 82 71 53 51 70 74 73 84 92 82 74 100 111 112 110 105 106 97 90 89 97 109 126 132 143 151 160 172 167 145 150 153 150 140 146 170 171 143 145 134 131 130 127 130 134 132 92 82 76 69 47 54 72 68 77 89 92 73 90 110 118 116 111 107 103 94 85 86 100 102 101 110 120 122 125 136 147 152 148 145 141 157 173 172 148 151 119 126 127 128 134 134 130 94 82 80 80 47 45 62 69 71 85 93 81 80 106 120 123 118 112 109 98 93 87 86 99 94 97 109 119 122 129 139 155 150 144 148 169 180 173 156 118 79 85 90 100 110 119 127 91 87 81 68 44 48 46 68 69 80 90 90 76 96 116 126 129 128 122 105 104 104 100 104 110 118 129 145 156 162 165 170 153 151 170 179 185 173 154 89 70 67 68 71 76 85 98 90 84 74 60 43 46 42 58 68 74 89 95 82 89 112 131 139 142 136 118 114 120 121 128 130 133 142 157 170 177 181 181 168 171 189 191 182 184 121 70 77 70 69 70 72 75 80 89 75 65 61 48 44 45 47 63 67 81 93 89 86 104 126 140 147 144 138 128 130 133 141 153 163 168 175 186 192 191 186 191 199 194 194 187 171 71 65 78 65 61 64 65 68 68 86 75 63 55 53 52 43 42 57 61 69 88 88 82 91 104 129 142 147 158 148 145 149 156 161 170 177 182 190 197 200 196 191 204 201 188 189 118 42 65 78 69 64 63 58 62 63\"')\n"
     ]
    }
   ],
   "source": [
    "for folder_name in os.listdir(train_folder_path):\n",
    "    folder_path = os.path.join(train_folder_path, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Use the folder name as the label for all the images in this folder\n",
    "        label = folder_name\n",
    "        label_num = label_map[label]\n",
    "        print(f'Processing images in folder \"{label}, {label_num}\"...')\n",
    "        \n",
    "        # Use another for loop to iterate over each file in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.jpg') or file_name.endswith('.jpeg') or file_name.endswith('.png'):\n",
    "                # Load the image using the PIL library\n",
    "                img = Image.open(os.path.join(folder_path, file_name))\n",
    "                size = img.size\n",
    "                \n",
    "                img_pixels_list = \"\"\n",
    "                for i in range (1, img.size[1]):\n",
    "                    for j in range (1, img.size[0]):\n",
    "                        coord = j,i \n",
    "                        img_pixels_list += str((img.getpixel(coord)))\n",
    "                        img_pixels_list += \" \"\n",
    "                        \n",
    "#                 img_pixels_list = img_pixels_list[:-1]\n",
    "#                 img_pixels_list += \"'\"\n",
    "                \n",
    "                new_string = '\"'\n",
    "                new_string += img_pixels_list[:-1]\n",
    "                new_string += '\"'\n",
    "                \n",
    "#                 # Apply the transformation to the image\n",
    "#                 img_tensor = transform(img)\n",
    "                # Append the resulting tensor to the list\n",
    "#                 train_tensors_list.append((label_num, img_tensor ))\n",
    "\n",
    "                train_pixels_list.append((label_num, new_string))\n",
    "    \n",
    "print(train_pixels_list[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef16e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_pixels_list, columns=['emotion', 'pixels'])\n",
    "df.to_csv('./fer2013/train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75dee72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images in folder \"happy, 3\"...\n",
      "Processing images in folder \"sad, 4\"...\n",
      "Processing images in folder \"fear, 2\"...\n",
      "Processing images in folder \"surprise, 5\"...\n",
      "Processing images in folder \"neutral, 6\"...\n",
      "Processing images in folder \"angry, 0\"...\n",
      "Processing images in folder \"disgust, 1\"...\n"
     ]
    }
   ],
   "source": [
    "for folder_name in os.listdir(test_folder_path):\n",
    "    folder_path = os.path.join(test_folder_path, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Use the folder name as the label for all the images in this folder\n",
    "        label = folder_name\n",
    "        label_num = label_map[label]\n",
    "        print(f'Processing images in folder \"{label}, {label_num}\"...')\n",
    "        \n",
    "        # Use another for loop to iterate over each file in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.jpg') or file_name.endswith('.jpeg') or file_name.endswith('.png'):\n",
    "                # Load the image using the PIL library\n",
    "                img = Image.open(os.path.join(folder_path, file_name))\n",
    "                size = img.size\n",
    "                \n",
    "                img_pixels_list = \"\"\n",
    "                for i in range (1, img.size[1]):\n",
    "                    for j in range (1, img.size[0]):\n",
    "                        coord = j,i \n",
    "                        img_pixels_list += str((img.getpixel(coord)))\n",
    "                        img_pixels_list += \" \"\n",
    "                \n",
    "                new_string = \"\\\"\"\n",
    "                new_string += img_pixels_list[:-1]\n",
    "                new_string += \"\\\"\"\n",
    "#                 img_pixels_list = img_pixels_list[:-1]\n",
    "#                 img_pixels_list += \"'\"\n",
    "                \n",
    "                # Append the resulting tensor to the list\n",
    "#                 test_tensors_list.append((label_num, img_tensor))\n",
    "                test_pixels_list.append((label_num, new_string))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17f5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_pixels_list, columns=['emotion', 'pixels'])\n",
    "df.to_csv('./fer2013/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28181f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31dce8f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "train.csv not found in biai-proj/fer2013 or corrupted. You can download it from https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m#how many units in each insert in the pipeline\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  declaring the training set as the train.csv file from the stated directory\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFER2013\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./biai-proj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# setting up the loader (aka iterator) for the train set \u001b[39;00m\n\u001b[1;32m      8\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      9\u001b[0m                                           shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/dev/biai/env/lib/python3.8/site-packages/torchvision/datasets/fer2013.py:44\u001b[0m, in \u001b[0;36mFER2013.__init__\u001b[0;34m(self, root, split, transform, target_transform)\u001b[0m\n\u001b[1;32m     42\u001b[0m data_file \u001b[38;5;241m=\u001b[39m base_folder \u001b[38;5;241m/\u001b[39m file_name\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(\u001b[38;5;28mstr\u001b[39m(data_file), md5\u001b[38;5;241m=\u001b[39mmd5):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or corrupted. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can download it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     52\u001b[0m         (\n\u001b[1;32m     53\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mint\u001b[39m(idx) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m48\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m csv\u001b[38;5;241m.\u001b[39mDictReader(file)\n\u001b[1;32m     57\u001b[0m     ]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: train.csv not found in biai-proj/fer2013 or corrupted. You can download it from https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge"
     ]
    }
   ],
   "source": [
    "batch_size = 10 #how many units in each insert in the pipeline\n",
    "\n",
    "#  declaring the training set as the train.csv file from the stated directory\n",
    "trainset = torchvision.datasets.FER2013(root='./biai-proj', split=\"train\",\n",
    "                                         transform=transform) \n",
    "\n",
    "# setting up the loader (aka iterator) for the train set \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "# repeating for the test set\n",
    "testset = torchvision.datasets.FER2013(root='./biai-proj', split=\"test\",\n",
    "                                        transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "classes = (\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be866faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
