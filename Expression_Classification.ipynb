{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6176ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "plt.ion() \n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d22df4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e22647e",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfff8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.0006, momentum=0.9)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 128 #how many units in each insert in the pipeline\n",
    "\n",
    "activation_func = F.relu\n",
    "# activation_func = F.sigmoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4b6ea53",
   "metadata": {},
   "source": [
    "## MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "268c6248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) \n",
    "    # output size = 6 *44*44 values \n",
    "    # image size : n*n \n",
    "    # filter size: f*f (f is odd number)\n",
    "    # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "    # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "     #output size = 18 * 18 * 16 = 5184   \n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(activation_func(self.conv1(x))) \n",
    "        # 44*44*6 , 22*22*6 \n",
    "        \n",
    "        x = self.pool(activation_func(self.conv2(x)))\n",
    "        # 18*18*16 , 9*9*16 \n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = activation_func(self.fc1(x))\n",
    "        x = activation_func(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88d61477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(net.parameters(), lr=0.0006, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbe04620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d8c1799-178e-400a-b147-0638c2f2c801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample #return a transformed image with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82970810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     class to transform to a normalized tensor (only the image pixel value is transformed)\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotions = sample['image'], sample['emotions']\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        return {'image': transform(image),\n",
    "                'emotions': emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "794873de-0f5b-41a4-8314-e73ce093e3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(os.path.join(train_folder_path,\"label.csv\"), train_folder_path, transform=ToTensor())\n",
    "valid_dataset = FERPlusDataset(os.path.join(valid_folder_path, \"label.csv\"), valid_folder_path, transform=ToTensor())\n",
    "test_dataset = FERPlusDataset(os.path.join(test_folder_path, \"label.csv\"), test_folder_path, transform=ToTensor())\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61f39aca-410d-4b30-b389-d2cdcc8d9170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # JUST FOR DEBUGGING \n",
    "# np_img = train_dataset[7]['image']\n",
    "# plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "# plt.show()\n",
    "\n",
    "# for i_batch, sample_batched in enumerate(trainloader):\n",
    "    # print(i_batch, sample_batched['image'].size(),\n",
    "    #       sample_batched['emotions'].size())\n",
    "\n",
    "    # # observe 4th batch and stop.\n",
    "    # if i_batch == 3:\n",
    "    #     print(sample_batched)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31425cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "Training Loss: 18.0381 | Training Accuracy: 10.86%\n",
      "Validation Loss: 16.5122 | Validation Accuracy: 14.65%\n",
      "-----------------------------------\n",
      "Epoch 2/100:\n",
      "Training Loss: 15.9856 | Training Accuracy: 24.46%\n",
      "Validation Loss: 15.6242 | Validation Accuracy: 19.26%\n",
      "-----------------------------------\n",
      "Epoch 3/100:\n",
      "Training Loss: 15.0391 | Training Accuracy: 27.46%\n",
      "Validation Loss: 14.4915 | Validation Accuracy: 31.16%\n",
      "-----------------------------------\n",
      "Epoch 4/100:\n",
      "Training Loss: 14.4031 | Training Accuracy: 30.16%\n",
      "Validation Loss: 14.0254 | Validation Accuracy: 31.69%\n",
      "-----------------------------------\n",
      "Epoch 5/100:\n",
      "Training Loss: 13.9590 | Training Accuracy: 32.03%\n",
      "Validation Loss: 13.5913 | Validation Accuracy: 33.37%\n",
      "-----------------------------------\n",
      "Epoch 6/100:\n",
      "Training Loss: 13.6103 | Training Accuracy: 33.54%\n",
      "Validation Loss: 13.2066 | Validation Accuracy: 32.48%\n",
      "-----------------------------------\n",
      "Epoch 7/100:\n",
      "Training Loss: 13.2950 | Training Accuracy: 35.04%\n",
      "Validation Loss: 12.9889 | Validation Accuracy: 33.37%\n",
      "-----------------------------------\n",
      "Epoch 8/100:\n",
      "Training Loss: 13.0142 | Training Accuracy: 36.41%\n",
      "Validation Loss: 12.8213 | Validation Accuracy: 33.59%\n",
      "-----------------------------------\n",
      "Epoch 9/100:\n",
      "Training Loss: 12.8123 | Training Accuracy: 37.47%\n",
      "Validation Loss: 12.6935 | Validation Accuracy: 35.33%\n",
      "-----------------------------------\n",
      "Epoch 10/100:\n",
      "Training Loss: 12.6176 | Training Accuracy: 38.22%\n",
      "Validation Loss: 12.5301 | Validation Accuracy: 36.86%\n",
      "-----------------------------------\n",
      "Epoch 11/100:\n",
      "Training Loss: 12.4200 | Training Accuracy: 39.20%\n",
      "Validation Loss: 12.6129 | Validation Accuracy: 34.40%\n",
      "-----------------------------------\n",
      "Epoch 12/100:\n",
      "Training Loss: 12.2320 | Training Accuracy: 40.08%\n",
      "Validation Loss: 12.4530 | Validation Accuracy: 35.86%\n",
      "-----------------------------------\n",
      "Epoch 13/100:\n",
      "Training Loss: 12.0635 | Training Accuracy: 40.94%\n",
      "Validation Loss: 12.5620 | Validation Accuracy: 38.21%\n",
      "-----------------------------------\n",
      "Epoch 14/100:\n",
      "Training Loss: 11.9057 | Training Accuracy: 41.81%\n",
      "Validation Loss: 12.5386 | Validation Accuracy: 39.30%\n",
      "-----------------------------------\n",
      "Epoch 15/100:\n",
      "Training Loss: 11.7523 | Training Accuracy: 42.45%\n",
      "Validation Loss: 12.5235 | Validation Accuracy: 38.04%\n",
      "-----------------------------------\n",
      "Epoch 16/100:\n",
      "Training Loss: 11.5825 | Training Accuracy: 43.45%\n",
      "Validation Loss: 12.4097 | Validation Accuracy: 38.65%\n",
      "-----------------------------------\n",
      "Epoch 17/100:\n",
      "Training Loss: 11.4473 | Training Accuracy: 43.94%\n",
      "Validation Loss: 12.5806 | Validation Accuracy: 37.84%\n",
      "-----------------------------------\n",
      "Epoch 18/100:\n",
      "Training Loss: 11.2692 | Training Accuracy: 44.72%\n",
      "Validation Loss: 12.5162 | Validation Accuracy: 37.67%\n",
      "-----------------------------------\n",
      "Epoch 19/100:\n",
      "Training Loss: 11.1687 | Training Accuracy: 45.45%\n",
      "Validation Loss: 12.5769 | Validation Accuracy: 38.51%\n",
      "-----------------------------------\n",
      "Epoch 20/100:\n",
      "Training Loss: 11.0194 | Training Accuracy: 45.92%\n",
      "Validation Loss: 12.5060 | Validation Accuracy: 39.60%\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "valid_loss = []\n",
    "valid_accuracy = []\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        labels = data['emotions'].to(device)\n",
    "        inputs = data['image'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # print(\"label before: \", labels)\n",
    "        # print(\"predicted before: \", outputs)\n",
    "        # Calculate and store training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        \n",
    "        # print(\"label: \", labels)\n",
    "        # print(\"predicted: \", predicted)\n",
    "        # print(\"pred size: \" , predicted.shape)\n",
    "        total += labels.size(0)\n",
    "        correct += (labels.bool() & (predicted == labels)).sum().item()\n",
    "        # print(\"1 more correct..\")\n",
    "    train_loss.append(running_loss / len(trainloader))\n",
    "    train_accuracy.append(100 * correct / total)\n",
    "    \n",
    "    # Perform validation\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            labels = data['emotions'].to(device)\n",
    "            images = data['image'].to(device)\n",
    "            \n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            # print(\"total: \" , total)\n",
    "            correct += (labels.bool() & (predicted == labels)).sum().item()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    valid_loss.append(running_loss / len(validloader))\n",
    "    valid_accuracy.append(100 * correct / total)\n",
    "    \n",
    "    # Print the training and validation loss and accuracy\n",
    "    print(f'Epoch {epoch+1}/{epochs}:')\n",
    "    print(f'Training Loss: {train_loss[-1]:.4f} | Training Accuracy: {train_accuracy[-1]:.2f}%')\n",
    "    print(f'Validation Loss: {valid_loss[-1]:.4f} | Validation Accuracy: {valid_accuracy[-1]:.2f}%')\n",
    "    print('-----------------------------------')\n",
    "\n",
    "elapsed_time = time.time() - st\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "print('Finished Training')\n",
    "\n",
    "# Plotting the loss and accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_loss, label='Training')\n",
    "plt.plot(range(1, epochs+1), valid_loss, label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracy, label='Training')\n",
    "plt.plot(range(1, epochs+1), valid_accuracy, label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
