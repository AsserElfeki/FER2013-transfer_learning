{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6176ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "plt.ion() \n",
    "plt.gray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e22647e",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfff8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 128 #how many units in each insert in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d22df4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbe04620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d8c1799-178e-400a-b147-0638c2f2c801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample #return a transformed image with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82970810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     class to transform to a normalized tensor (only the image pixel value is transformed)\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotions = sample['image'], sample['emotions']\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        return {'image': transform(image),\n",
    "                'emotions': emotions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "794873de-0f5b-41a4-8314-e73ce093e3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(os.path.join(train_folder_path,\"label.csv\"), train_folder_path, transform=ToTensor())\n",
    "valid_dataset = FERPlusDataset(os.path.join(valid_folder_path, \"label.csv\"), valid_folder_path, transform=ToTensor())\n",
    "test_dataset = FERPlusDataset(os.path.join(test_folder_path, \"label.csv\"), test_folder_path, transform=ToTensor())\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61f39aca-410d-4b30-b389-d2cdcc8d9170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([128, 1, 48, 48]) torch.Size([128, 10])\n",
      "1 torch.Size([128, 1, 48, 48]) torch.Size([128, 10])\n",
      "2 torch.Size([128, 1, 48, 48]) torch.Size([128, 10])\n",
      "3 torch.Size([128, 1, 48, 48]) torch.Size([128, 10])\n",
      "{'image': tensor([[[[0.1176, 0.1255, 0.1373,  ..., 0.1804, 0.1529, 0.1412],\n",
      "          [0.1255, 0.1176, 0.1137,  ..., 0.1882, 0.1647, 0.1529],\n",
      "          [0.1059, 0.1333, 0.1216,  ..., 0.1961, 0.1647, 0.1608],\n",
      "          ...,\n",
      "          [0.0667, 0.0784, 0.0863,  ..., 0.1412, 0.1569, 0.1333],\n",
      "          [0.0941, 0.0824, 0.0667,  ..., 0.1686, 0.1569, 0.1255],\n",
      "          [0.0824, 0.0824, 0.0863,  ..., 0.1647, 0.1529, 0.1333]]],\n",
      "\n",
      "\n",
      "        [[[0.7569, 0.7686, 0.7843,  ..., 0.9922, 0.9961, 0.9961],\n",
      "          [0.7529, 0.7686, 0.7843,  ..., 0.9882, 0.9961, 0.9961],\n",
      "          [0.7529, 0.7647, 0.7765,  ..., 0.9882, 0.9961, 0.9961],\n",
      "          ...,\n",
      "          [0.9373, 0.9216, 0.9451,  ..., 0.9922, 0.9961, 0.9922],\n",
      "          [0.9490, 0.9373, 0.9373,  ..., 0.9922, 0.9882, 0.9882],\n",
      "          [0.9490, 0.8863, 0.9255,  ..., 0.9608, 0.9725, 0.9608]]],\n",
      "\n",
      "\n",
      "        [[[0.9961, 1.0000, 0.9176,  ..., 0.8588, 1.0000, 0.9961],\n",
      "          [0.9961, 1.0000, 0.9294,  ..., 0.8941, 1.0000, 0.9961],\n",
      "          [1.0000, 1.0000, 0.9451,  ..., 0.8431, 1.0000, 0.9922],\n",
      "          ...,\n",
      "          [0.9961, 1.0000, 0.8980,  ..., 0.8784, 1.0000, 0.9961],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.8157, 0.3412, 0.3765,  ..., 0.9333, 0.9333, 0.9373],\n",
      "          [0.6353, 0.3373, 0.3765,  ..., 0.9333, 0.9373, 0.9412],\n",
      "          [0.4392, 0.3686, 0.3412,  ..., 0.9333, 0.9373, 0.9412],\n",
      "          ...,\n",
      "          [0.1176, 0.1137, 0.1216,  ..., 0.9020, 0.9059, 0.9059],\n",
      "          [0.1216, 0.1137, 0.1255,  ..., 0.9020, 0.9020, 0.9059],\n",
      "          [0.1216, 0.1216, 0.1294,  ..., 0.8824, 0.9059, 0.9059]]],\n",
      "\n",
      "\n",
      "        [[[0.1725, 0.2196, 0.3412,  ..., 0.3725, 0.4039, 0.4118],\n",
      "          [0.1647, 0.2314, 0.3020,  ..., 0.3922, 0.4000, 0.4118],\n",
      "          [0.1686, 0.2118, 0.3059,  ..., 0.4000, 0.3569, 0.4039],\n",
      "          ...,\n",
      "          [0.8039, 0.7804, 0.8314,  ..., 0.7608, 0.8471, 0.9059],\n",
      "          [0.7725, 0.7490, 0.7333,  ..., 0.8000, 0.7804, 0.8471],\n",
      "          [0.6392, 0.6863, 0.7059,  ..., 0.8314, 0.8039, 0.8667]]],\n",
      "\n",
      "\n",
      "        [[[0.9961, 1.0000, 0.8980,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 0.9608, 0.7569,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 0.8392, 0.6667,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.7255, 0.6039, 0.6039,  ..., 0.4980, 1.0000, 0.9882],\n",
      "          [0.7137, 0.5608, 0.6196,  ..., 0.4118, 0.9686, 1.0000],\n",
      "          [0.6941, 0.5490, 0.5961,  ..., 0.3333, 0.8824, 1.0000]]]]), 'emotions': tensor([[ 9.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  9.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 3.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 8.,  0.,  1.,  ...,  0.,  1.,  0.],\n",
      "        [ 0., 10.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  8.,  2.,  ...,  0.,  0.,  0.]])}\n"
     ]
    }
   ],
   "source": [
    "# JUST FOR DEBUGGING \n",
    "np_img = train_dataset[7]['image']\n",
    "plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "for i_batch, sample_batched in enumerate(trainloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['emotions'].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        print(sample_batched)\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5af5b92d",
   "metadata": {},
   "source": [
    "**the MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1536c39d-bb61-4f46-9ff2-e9c15dcfaa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "activation_func = F.relu\n",
    "# activation_func = F.sigmoid\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) \n",
    "    # output size = 6 *44*44 values \n",
    "    # image size : n*n \n",
    "    # filter size: f*f (f is odd number)\n",
    "    # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "    # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "     #output size = 18 * 18 * 16 = 5184   \n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(activation_func(self.conv1(x))) \n",
    "        # 44*44*6 , 22*22*6 \n",
    "        \n",
    "        x = self.pool(activation_func(self.conv2(x)))\n",
    "        # 18*18*16 , 9*9*16 \n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = activation_func(self.fc1(x))\n",
    "        x = activation_func(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e0ec98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.0006, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31425cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:\n",
      "Training Loss: 18.0228 | Training Accuracy: 12.81%\n",
      "Validation Loss: 16.7048 | Validation Accuracy: 23.62%\n",
      "-----------------------------------\n",
      "Epoch 2/50:\n",
      "Training Loss: 16.3157 | Training Accuracy: 23.62%\n",
      "Validation Loss: 15.5598 | Validation Accuracy: 23.50%\n",
      "-----------------------------------\n",
      "Epoch 3/50:\n",
      "Training Loss: 15.3613 | Training Accuracy: 26.66%\n",
      "Validation Loss: 15.2116 | Validation Accuracy: 30.49%\n",
      "-----------------------------------\n",
      "Epoch 4/50:\n",
      "Training Loss: 14.7637 | Training Accuracy: 28.79%\n",
      "Validation Loss: 14.5877 | Validation Accuracy: 30.38%\n",
      "-----------------------------------\n",
      "Epoch 5/50:\n",
      "Training Loss: 14.3259 | Training Accuracy: 30.40%\n",
      "Validation Loss: 13.8676 | Validation Accuracy: 31.27%\n",
      "-----------------------------------\n",
      "Epoch 6/50:\n",
      "Training Loss: 13.9419 | Training Accuracy: 31.94%\n",
      "Validation Loss: 13.7221 | Validation Accuracy: 29.21%\n",
      "-----------------------------------\n",
      "Epoch 7/50:\n",
      "Training Loss: 13.6024 | Training Accuracy: 33.64%\n",
      "Validation Loss: 13.4917 | Validation Accuracy: 32.53%\n",
      "-----------------------------------\n",
      "Epoch 8/50:\n",
      "Training Loss: 13.3430 | Training Accuracy: 35.07%\n",
      "Validation Loss: 13.1188 | Validation Accuracy: 31.58%\n",
      "-----------------------------------\n",
      "Epoch 9/50:\n",
      "Training Loss: 13.0924 | Training Accuracy: 36.23%\n",
      "Validation Loss: 12.8854 | Validation Accuracy: 35.30%\n",
      "-----------------------------------\n",
      "Epoch 10/50:\n",
      "Training Loss: 12.8730 | Training Accuracy: 37.21%\n",
      "Validation Loss: 12.7812 | Validation Accuracy: 36.31%\n",
      "-----------------------------------\n",
      "Epoch 11/50:\n",
      "Training Loss: 12.6977 | Training Accuracy: 38.07%\n",
      "Validation Loss: 12.6605 | Validation Accuracy: 35.30%\n",
      "-----------------------------------\n",
      "Epoch 12/50:\n",
      "Training Loss: 12.4643 | Training Accuracy: 38.97%\n",
      "Validation Loss: 12.6428 | Validation Accuracy: 34.07%\n",
      "-----------------------------------\n",
      "Epoch 13/50:\n",
      "Training Loss: 12.3118 | Training Accuracy: 39.84%\n",
      "Validation Loss: 12.6726 | Validation Accuracy: 34.57%\n",
      "-----------------------------------\n",
      "Epoch 14/50:\n",
      "Training Loss: 12.1764 | Training Accuracy: 40.57%\n",
      "Validation Loss: 12.6445 | Validation Accuracy: 37.90%\n",
      "-----------------------------------\n",
      "Epoch 15/50:\n",
      "Training Loss: 12.0091 | Training Accuracy: 41.44%\n",
      "Validation Loss: 12.7537 | Validation Accuracy: 37.09%\n",
      "-----------------------------------\n",
      "Epoch 16/50:\n",
      "Training Loss: 11.8858 | Training Accuracy: 42.02%\n",
      "Validation Loss: 12.5579 | Validation Accuracy: 35.97%\n",
      "-----------------------------------\n",
      "Epoch 17/50:\n",
      "Training Loss: 11.7698 | Training Accuracy: 42.41%\n",
      "Validation Loss: 12.4760 | Validation Accuracy: 38.54%\n",
      "-----------------------------------\n",
      "Epoch 18/50:\n",
      "Training Loss: 11.6282 | Training Accuracy: 43.23%\n",
      "Validation Loss: 12.4390 | Validation Accuracy: 37.37%\n",
      "-----------------------------------\n",
      "Epoch 19/50:\n",
      "Training Loss: 11.5368 | Training Accuracy: 43.79%\n",
      "Validation Loss: 12.4692 | Validation Accuracy: 38.93%\n",
      "-----------------------------------\n",
      "Epoch 20/50:\n",
      "Training Loss: 11.4082 | Training Accuracy: 44.28%\n",
      "Validation Loss: 12.4457 | Validation Accuracy: 37.67%\n",
      "-----------------------------------\n",
      "Epoch 21/50:\n",
      "Training Loss: 11.3052 | Training Accuracy: 44.45%\n",
      "Validation Loss: 12.3243 | Validation Accuracy: 38.40%\n",
      "-----------------------------------\n",
      "Epoch 22/50:\n",
      "Training Loss: 11.2077 | Training Accuracy: 45.09%\n",
      "Validation Loss: 12.6007 | Validation Accuracy: 38.46%\n",
      "-----------------------------------\n",
      "Epoch 23/50:\n",
      "Training Loss: 11.0897 | Training Accuracy: 45.60%\n",
      "Validation Loss: 12.4264 | Validation Accuracy: 37.37%\n",
      "-----------------------------------\n",
      "Epoch 24/50:\n",
      "Training Loss: 10.9797 | Training Accuracy: 46.05%\n",
      "Validation Loss: 12.3415 | Validation Accuracy: 37.45%\n",
      "-----------------------------------\n",
      "Epoch 25/50:\n",
      "Training Loss: 10.8878 | Training Accuracy: 46.42%\n",
      "Validation Loss: 12.5812 | Validation Accuracy: 39.04%\n",
      "-----------------------------------\n",
      "Epoch 26/50:\n",
      "Training Loss: 10.8221 | Training Accuracy: 46.83%\n",
      "Validation Loss: 12.5894 | Validation Accuracy: 37.45%\n",
      "-----------------------------------\n",
      "Epoch 27/50:\n",
      "Training Loss: 10.7345 | Training Accuracy: 47.12%\n",
      "Validation Loss: 12.5158 | Validation Accuracy: 38.79%\n",
      "-----------------------------------\n",
      "Epoch 28/50:\n",
      "Training Loss: 10.6669 | Training Accuracy: 47.57%\n",
      "Validation Loss: 12.5874 | Validation Accuracy: 38.74%\n",
      "-----------------------------------\n",
      "Epoch 29/50:\n",
      "Training Loss: 10.6093 | Training Accuracy: 47.69%\n",
      "Validation Loss: 12.6280 | Validation Accuracy: 39.13%\n",
      "-----------------------------------\n",
      "Epoch 30/50:\n",
      "Training Loss: 10.5158 | Training Accuracy: 48.23%\n",
      "Validation Loss: 12.6375 | Validation Accuracy: 37.53%\n",
      "-----------------------------------\n",
      "Epoch 31/50:\n",
      "Training Loss: 10.4569 | Training Accuracy: 48.38%\n",
      "Validation Loss: 12.7564 | Validation Accuracy: 38.65%\n",
      "-----------------------------------\n",
      "Epoch 32/50:\n",
      "Training Loss: 10.3852 | Training Accuracy: 48.72%\n",
      "Validation Loss: 12.9813 | Validation Accuracy: 38.88%\n",
      "-----------------------------------\n",
      "Epoch 33/50:\n",
      "Training Loss: 10.2998 | Training Accuracy: 49.25%\n",
      "Validation Loss: 12.8234 | Validation Accuracy: 38.93%\n",
      "-----------------------------------\n",
      "Epoch 34/50:\n",
      "Training Loss: 10.2717 | Training Accuracy: 49.23%\n",
      "Validation Loss: 12.7958 | Validation Accuracy: 38.35%\n",
      "-----------------------------------\n",
      "Epoch 35/50:\n",
      "Training Loss: 10.1779 | Training Accuracy: 49.65%\n",
      "Validation Loss: 12.9408 | Validation Accuracy: 38.79%\n",
      "-----------------------------------\n",
      "Epoch 36/50:\n",
      "Training Loss: 10.1150 | Training Accuracy: 50.01%\n",
      "Validation Loss: 13.1113 | Validation Accuracy: 38.88%\n",
      "-----------------------------------\n",
      "Epoch 37/50:\n",
      "Training Loss: 10.0858 | Training Accuracy: 50.03%\n",
      "Validation Loss: 13.0977 | Validation Accuracy: 37.81%\n",
      "-----------------------------------\n",
      "Epoch 38/50:\n",
      "Training Loss: 10.0488 | Training Accuracy: 50.44%\n",
      "Validation Loss: 13.0064 | Validation Accuracy: 38.43%\n",
      "-----------------------------------\n",
      "Epoch 39/50:\n",
      "Training Loss: 9.9588 | Training Accuracy: 50.78%\n",
      "Validation Loss: 13.1772 | Validation Accuracy: 38.79%\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "valid_loss = []\n",
    "valid_accuracy = []\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        labels = data['emotions'].to(device)\n",
    "        inputs = data['image'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # print(\"label before: \", labels)\n",
    "        # print(\"predicted before: \", outputs)\n",
    "        # Calculate and store training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        \n",
    "        # print(\"label: \", labels)\n",
    "        # print(\"predicted: \", predicted)\n",
    "        # print(\"pred size: \" , predicted.shape)\n",
    "        total += labels.size(0)\n",
    "        correct += (labels.bool() & (predicted == labels)).sum().item()\n",
    "        # print(\"1 more correct..\")\n",
    "    train_loss.append(running_loss / len(trainloader))\n",
    "    train_accuracy.append(100 * correct / total)\n",
    "    \n",
    "    # Perform validation\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            labels = data['emotions'].to(device)\n",
    "            images = data['image'].to(device)\n",
    "            \n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            # print(\"total: \" , total)\n",
    "            correct += (labels.bool() & (predicted == labels)).sum().item()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    valid_loss.append(running_loss / len(validloader))\n",
    "    valid_accuracy.append(100 * correct / total)\n",
    "    \n",
    "    # Print the training and validation loss and accuracy\n",
    "    print(f'Epoch {epoch+1}/{epochs}:')\n",
    "    print(f'Training Loss: {train_loss[-1]:.4f} | Training Accuracy: {train_accuracy[-1]:.2f}%')\n",
    "    print(f'Validation Loss: {valid_loss[-1]:.4f} | Validation Accuracy: {valid_accuracy[-1]:.2f}%')\n",
    "    print('-----------------------------------')\n",
    "\n",
    "elapsed_time = time.time() - st\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "print('Finished Training')\n",
    "\n",
    "# Plotting the loss and accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_loss, label='Training')\n",
    "plt.plot(range(1, epochs+1), valid_loss, label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracy, label='Training')\n",
    "plt.plot(range(1, epochs+1), valid_accuracy, label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e50abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = time.time()\n",
    "\n",
    "# for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         labels = data['emotions']\n",
    "#         inputs = data['image']\n",
    "#         # print(\"labels\" , labels)\n",
    "#         # print(\"inputs\" , inputs)\n",
    "        \n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = net(inputs)\n",
    "#         # print(\"outputs\" , outputs.shape)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         # print(\"loss\", loss)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "#             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "#             running_loss = 0.0\n",
    "    \n",
    "# elapsed_time = time.time() - st\n",
    "# print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "# print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# i = 0\n",
    "# # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "# with torch.no_grad():\n",
    "        \n",
    "#     for data in testloader: \n",
    "#         i+= 1   \n",
    "#         if i == 5:\n",
    "#                 print(\"data: \" , data['emotions'])    \n",
    "#                 print(\"data: \" , data['image'])\n",
    "        \n",
    "#         labels = data['emotions']\n",
    "#         images = data['image']\n",
    "#         # print labels and images \n",
    "        \n",
    "        \n",
    "#         # calculate outputs by running images through the network\n",
    "#         outputs = net(images)\n",
    "\n",
    "#         # the class with the highest energy is what we choose as prediction\n",
    "#         _, labels = torch.max(labels, 1)\n",
    "#         # print(\"labels size: \", labels.shape)\n",
    "        \n",
    "# #         print(\"labels after max: \", labels)\n",
    "        \n",
    "# #         print (type(labels) , \" \" , labels)    \n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         # print(\"pred size: \", predicted.shape)\n",
    "#         total += labels.size(0)\n",
    "# #         print(labels.size(0))\n",
    "# #         print(\"prediction shape: \", predicted.shape , \"label shape: \" , labels.shape)\n",
    "#         correct += (labels.bool() & (predicted == labels)).sum().item()\n",
    "\n",
    "# print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4e51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptimization parameters : \\nepochs, batch size, lr (scheduler), optimizer, loss function, activation function, arch. \\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "optimization parameters : \n",
    "epochs, batch size, lr (scheduler), optimizer, loss function, activation function, arch. \n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
