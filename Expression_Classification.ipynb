{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6176ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.mps \n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "# plt.ion() \n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22df4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8c1799-178e-400a-b147-0638c2f2c801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample #return a transformed image with label\n",
    "    \n",
    "    def to(self, device):\n",
    "        # move to device\n",
    "        return self.to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82970810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     class to transform to a normalized tensor (only the image pixel value is transformed)\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotions = sample['image'], sample['emotions']\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        return {'image': transform(image),\n",
    "                'emotions': emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe04620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794873de-0f5b-41a4-8314-e73ce093e3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(os.path.join(train_folder_path,\"label.csv\"), train_folder_path, transform=ToTensor())\n",
    "valid_dataset = FERPlusDataset(os.path.join(valid_folder_path, \"label.csv\"), valid_folder_path, transform=ToTensor())\n",
    "test_dataset = FERPlusDataset(os.path.join(test_folder_path, \"label.csv\"), test_folder_path, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1d9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_path = 'stats/outputs3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f097f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_table (epochs, optimizer ,criterion, batch_size, learning_rate, activation_func, elapsed_time, train_accuracy, valid_accuracy, id, scheduler, device_name): \n",
    "    with open(f'{outputs_path}/statistics.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # if csvfile.tell() == 0:\n",
    "        #     # Write the column headers\n",
    "        #     writer.writerow(['trial', 'Batch size', 'Epochs', 'Activation function', 'Loss function', 'Initial Learning rate', 'Optimizer', 'Scheduler',\n",
    "        #                      'Min training accuracy', 'Max training accuracy', 'Average training accuracy',\n",
    "        #                      'Min validation accuracy', 'Max validation accuracy', 'Average validation accuracy',\n",
    "        #                      'Total time'])\n",
    "\n",
    "        # Write the row of data\n",
    "        writer.writerow([id, batch_size, epochs, activation_func, criterion, learning_rate, optimizer, scheduler,\n",
    "                         round(min(train_accuracy), 2) , round(max(train_accuracy), 2), round(sum(train_accuracy) / len(train_accuracy), 2),\n",
    "                         round(min(valid_accuracy), 2), round(max(valid_accuracy), 2), round(sum(valid_accuracy) / len(valid_accuracy), 2),\n",
    "                         time.strftime('%H:%M:%S', time.gmtime(elapsed_time)), device_name])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c4fa0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(epochs, train_accuracy, valid_accuracy, train_loss, valid_loss):      \n",
    "    # Plotting the loss and accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), train_loss, label='Training')\n",
    "    plt.plot(range(1, epochs+1), valid_loss, label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), train_accuracy, label='Training')\n",
    "    plt.plot(range(1, epochs+1), valid_accuracy, label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be470a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_details_to_text (epochs, optimizer, scheduler ,criterion, batch_size, learning_rate, id):    \n",
    "    # Create a file to write the output\n",
    "    filename = f'{outputs_path}/trial_details/trial_{id+24}'\n",
    "    output_file = open(filename, \"w\")\n",
    "\n",
    "    output_file.write(f\"Parameter Combination: \\n\")\n",
    "    output_file.write(f\"epochs: {epochs} \\n\")\n",
    "    output_file.write(f\"initial learning_rate: {learning_rate} \\n\")\n",
    "    output_file.write(f\"batch_size: {batch_size} \\n\")\n",
    "    output_file.write(f\"optimizer: {optimizer} \\n\")\n",
    "    output_file.write(f\"scheduler: {scheduler} \\n\")\n",
    "    output_file.write(f\"criterion: {criterion} \\n\")\n",
    "    output_file.write(f\"\\n\")\n",
    "    \n",
    "    output_file.write(f\"Finished Training with this combination\\n\")\n",
    "    \n",
    "    output_file.write(\"#\"*70)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31425cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, scheduler ,criterion, model, trainloader, validloader, batch_size, learning_rate, activation_func, iteratior_id, device):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []\n",
    "    opt_name = optimizer.__name__\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if scheduler == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = scheduler(optimizer)\n",
    "        # print(\"plateu\")\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    elif scheduler == optim.lr_scheduler.ExponentialLR: \n",
    "        scheduler = scheduler(optimizer, gamma=0.9)\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    st = time.time()\n",
    "\n",
    "# Training - Validation loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        model.train()\n",
    "        # Perform training\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # labels = data['emotions']\n",
    "            # inputs = data['image']\n",
    "            labels = data['emotions'].to(device)\n",
    "            inputs = data['image'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # print(\"label before: \", labels)\n",
    "            # print(\"predicted before: \", outputs)\n",
    "            # Calculate and store training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            \n",
    "            # print(\"label: \", labels)\n",
    "            # print(\"predicted: \", predicted)\n",
    "            # print(\"pred size: \" , predicted.shape)\n",
    "            total += labels.size(0)\n",
    "            correct +=  (predicted == labels).sum().item()\n",
    "            # print(\"1 more correct..\")\n",
    "        \n",
    "        if type(scheduler) == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "            scheduler.step(running_loss / len(validloader))\n",
    "            \n",
    "        else:        \n",
    "            scheduler.step()\n",
    "                \n",
    "        train_loss.append(running_loss / len(trainloader))\n",
    "        train_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Perform validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for data in validloader:\n",
    "                # labels = data['emotions']\n",
    "                # images = data['image']\n",
    "                labels = data['emotions'].to(device)\n",
    "                images = data['image'].to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                # print(\"total: \" , total)\n",
    "                correct +=  (predicted == labels).sum().item() #can be torch.eq(pred, labels).sum().item()\n",
    "        \n",
    "        valid_loss.append(running_loss / len(validloader))\n",
    "        valid_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Print the training and validation loss and accuracy\n",
    "        # print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        # print(f'Training Loss: {train_loss[-1]:.4f} | Training Accuracy: {train_accuracy[-1]:.2f}%')\n",
    "        # print(f'Validation Loss: {valid_loss[-1]:.4f} | Validation Accuracy: {valid_accuracy[-1]:.2f}%')\n",
    "        # print('-----------------------------------')\n",
    "\n",
    "    elapsed_time = time.time() - st\n",
    "    # print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "    # print('Finished Training')\n",
    "    # with open(f'{outputs_path}/statistics.csv', 'a', newline='') as csvfile:\n",
    "    #     writer = csv.writer(csvfile)\n",
    "\n",
    "    #     if csvfile.tell() == 0:\n",
    "    #         # Write the column headers\n",
    "    #         writer.writerow(['trial', 'Batch size', 'Epochs', 'Activation function', 'Loss function', 'Initial Learning rate', 'Optimizer', 'Scheduler',\n",
    "    #                         'Min training accuracy', 'Max training accuracy', 'Average training accuracy',\n",
    "    #                         'Min validation accuracy', 'Max validation accuracy', 'Average validation accuracy',\n",
    "    #                         'Total time', 'device'])\n",
    "\n",
    "    if device == torch.device(\"cpu\"):\n",
    "            device_name = 'CPU'\n",
    "    elif device == torch.device(\"mps\"):\n",
    "            device_name = 'MPS'\n",
    "            \n",
    "    write_to_table(epochs, opt_name, criterion, batch_size, learning_rate, activation_func.__name__, elapsed_time, train_accuracy, valid_accuracy, iteratior_id, scheduler.__class__.__name__, device_name)\n",
    "    \n",
    "    create_plot(epochs, train_accuracy, valid_accuracy, train_loss, valid_loss)\n",
    "    \n",
    "    output_details_to_text(epochs, optimizer, scheduler.__class__.__name__, criterion, batch_size, learning_rate, iteratior_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ad6e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "criterions = [nn.CrossEntropyLoss()]\n",
    "optimizers = [optim.SGD, optim.Adam]\n",
    "activations = [F.relu]\n",
    "learning_rates = [0.01]\n",
    "epochs = [50]\n",
    "batch_size = [ 32]\n",
    "schedulers = [optim.lr_scheduler.ExponentialLR, optim.lr_scheduler.ReduceLROnPlateau]\n",
    "device = [torch.device(\"mps\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f92ff2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all possible parameter combinations\n",
    "parameter_grid = itertools.product(learning_rates, batch_size, epochs, schedulers, optimizers, activations, criterions, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76aafcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, params in enumerate(parameter_grid):\n",
    "    learning_rate, batch_size, epochs, scheduler, optimizer, activation, criterion, device = params \n",
    "    # print all params\n",
    "    # print(params)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    # testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Build the CNN model with the given parameters\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, drop=0.2):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) \n",
    "        # output size = 6 *44*44 values \n",
    "        # image size : n*n \n",
    "        # filter size: f*f (f is odd number)\n",
    "        # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "            self.bn1 = nn.BatchNorm2d(6)  # Batch normalization after conv1\n",
    "            \n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "        # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "        # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "            \n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #output size = 18 * 18 * 16 = 5184   \n",
    "            \n",
    "            self.bn2 = nn.BatchNorm2d(16)  # Batch normalization after conv2\n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "            \n",
    "            self.dropout = nn.Dropout(p=drop)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.pool(activation(self.bn1(self.conv1(x)))) \n",
    "            # 44*44*6 , 22*22*6 \n",
    "            \n",
    "            x = self.pool(activation(self.bn2(self.conv2(x))))\n",
    "            # 18*18*16 , 9*9*16 \n",
    "            \n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = activation(self.dropout(self.fc1(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = activation(self.dropout(self.fc2(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    torch.mps.manual_seed(42) #do I need it ? wasn't in the current stats\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    train_and_validate(epochs, optimizer, scheduler , criterion, model, trainloader, validloader, batch_size, learning_rate, activation, i, device)\n",
    "    plt.savefig(f\"{outputs_path}/plots/trial{i+24} act_func={activation.__name__}, loss_func={criterion.__class__.__name__}, opt={optimizer.__name__}, LRS={scheduler.__name__} .png\")\n",
    "    plt.close()\n",
    "    plt.close('all')\n",
    "    # make sure there is no memory leaks .. not working as expected \n",
    "    torch.mps.empty_cache()\n",
    "    # clear ram\n",
    "    del model\n",
    "    # free memory\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    # free dataloader memory \n",
    "    del trainloader\n",
    "    del validloader\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82778be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from pathlib import Path \n",
    "\n",
    "# # Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "# if Path(\"helper_functions.py\").is_file():\n",
    "#   print(\"helper_functions.py already exists, skipping download\")\n",
    "# else:\n",
    "#   print(\"Downloading helper_functions.py\")\n",
    "#   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "#   with open(\"helper_functions.py\", \"wb\") as f:\n",
    "#     f.write(request.content)\n",
    "\n",
    "# from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
