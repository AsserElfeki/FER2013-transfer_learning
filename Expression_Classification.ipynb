{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6176ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "# plt.ion() \n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22df4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8c1799-178e-400a-b147-0638c2f2c801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample #return a transformed image with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82970810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     class to transform to a normalized tensor (only the image pixel value is transformed)\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotions = sample['image'], sample['emotions']\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        return {'image': transform(image),\n",
    "                'emotions': emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe04620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794873de-0f5b-41a4-8314-e73ce093e3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(os.path.join(train_folder_path,\"label.csv\"), train_folder_path, transform=ToTensor())\n",
    "valid_dataset = FERPlusDataset(os.path.join(valid_folder_path, \"label.csv\"), valid_folder_path, transform=ToTensor())\n",
    "test_dataset = FERPlusDataset(os.path.join(test_folder_path, \"label.csv\"), test_folder_path, transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31425cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, scheduler ,criterion, model, trainloader, validloader, batch_size, learning_rate):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if scheduler == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = scheduler(optimizer)\n",
    "        # print(\"plateu\")\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    elif scheduler == optim.lr_scheduler.ExponentialLR: \n",
    "        scheduler = scheduler(optimizer, gamma=0.9)\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    st = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            \n",
    "            labels = data['emotions'].to(device)\n",
    "            inputs = data['image'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            # print(\"label before: \", labels)\n",
    "            # print(\"predicted before: \", outputs)\n",
    "            # Calculate and store training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            \n",
    "            # print(\"label: \", labels)\n",
    "            # print(\"predicted: \", predicted)\n",
    "            # print(\"pred size: \" , predicted.shape)\n",
    "            total += labels.size(0)\n",
    "            correct += (labels.bool() & (predicted == labels)).sum().item()\n",
    "            # print(\"1 more correct..\")\n",
    "            \n",
    "        train_loss.append(running_loss / len(trainloader))\n",
    "        train_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Perform validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in validloader:\n",
    "                labels = data['emotions'].to(device)\n",
    "                images = data['image'].to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                # print(\"total: \" , total)\n",
    "                correct += (labels.bool() & (predicted == labels)).sum().item()\n",
    "        \n",
    "        # print(type(scheduler))\n",
    "                \n",
    "        if type(scheduler) == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "            scheduler.step(running_loss / len(validloader))\n",
    "            # print(\"plateu 2\")\n",
    "            \n",
    "        else:        \n",
    "            scheduler.step()\n",
    "            # print(\"other 2\")\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        valid_loss.append(running_loss / len(validloader))\n",
    "        valid_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Print the training and validation loss and accuracy\n",
    "        # print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        # print(f'Training Loss: {train_loss[-1]:.4f} | Training Accuracy: {train_accuracy[-1]:.2f}%')\n",
    "        # print(f'Validation Loss: {valid_loss[-1]:.4f} | Validation Accuracy: {valid_accuracy[-1]:.2f}%')\n",
    "        # print('-----------------------------------')\n",
    "\n",
    "    elapsed_time = time.time() - st\n",
    "    # print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "    # print('Finished Training')\n",
    "\n",
    "    # Plotting the loss and accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), train_loss, label='Training')\n",
    "    plt.plot(range(1, epochs+1), valid_loss, label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), train_accuracy, label='Training')\n",
    "    plt.plot(range(1, epochs+1), valid_accuracy, label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create a file to write the output\n",
    "    output_file = open(\"output.txt\", \"a\")\n",
    "\n",
    "    output_file.write(f\"Parameter Combination: \\n\")\n",
    "    output_file.write(f\"epochs: {epochs} \\n\")\n",
    "    output_file.write(f\"learning_rate: {learning_rate} \\n\")\n",
    "    output_file.write(f\"batch_size: {batch_size} \\n\")\n",
    "    output_file.write(f\"optimizer: {optimizer} \\n\")\n",
    "    output_file.write(f\"scheduler: {scheduler} \\n\")\n",
    "    output_file.write(f\"criterion: {criterion} \\n\")\n",
    "    output_file.write(f\"\\n\")\n",
    "    \n",
    "    output_file.write(\"-\"*50)\n",
    "    output_file.write(f\"\\n\")\n",
    "    \n",
    "    output_file.write(f\"Training Loss: \\n\")\n",
    "    output_file.write(f\"Min: {min(train_loss)}\\n\")\n",
    "    output_file.write(f\"Max: {max(train_loss)}\\n\")\n",
    "    output_file.write(f\"Average: {sum(train_loss)/len(train_loss)}\\n\")\n",
    "    output_file.write(\"\\n\")\n",
    "    \n",
    "    output_file.write(\"-\"*50)\n",
    "    output_file.write(f\"\\n\")\n",
    "    \n",
    "    output_file.write(f\"Training Accuracy: \\n\")\n",
    "    # output min, max, average training accuracy\n",
    "    output_file.write(f\"Min: {min(train_accuracy)}\\n\")\n",
    "    output_file.write(f\"Max: {max(train_accuracy)}\\n\")\n",
    "    output_file.write(f\"Average: {sum(train_accuracy)/len(train_accuracy)}\\n\")\n",
    "    output_file.write(\"\\n\")\n",
    "    \n",
    "    output_file.write(\"-\"*50)\n",
    "    output_file.write(f\"\\n\")\n",
    "    \n",
    "    output_file.write(f\"Validation Loss: \\n\")\n",
    "    output_file.write(f\"Min: {min(valid_loss)}\\n\")\n",
    "    output_file.write(f\"Max: {max(valid_loss)}\\n\")\n",
    "    output_file.write(f\"Average: {sum(valid_loss)/len(valid_loss)}\\n\")\n",
    "    output_file.write(\"\\n\")\n",
    "    \n",
    "    output_file.write(\"-\"*50)\n",
    "    output_file.write(f\"\\n\")\n",
    "    \n",
    "    output_file.write(f\"Validation Accuracy: \\n\")\n",
    "    output_file.write(f\"Min: {min(valid_accuracy)}\\n\")\n",
    "    output_file.write(f\"Max: {max(valid_accuracy)}\\n\")\n",
    "    output_file.write(f\"Average: {sum(valid_accuracy)/len(valid_accuracy)}\\n\")\n",
    "    output_file.write(\"\\n\")\n",
    "    \n",
    "    output_file.write(f\"Execution time: {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}\\n\")\n",
    "    output_file.write(f\"\\n\")\n",
    "    output_file.write(f\"Finished Training with this combination\\n\")\n",
    "    \n",
    "    output_file.write(\"#\"*70)\n",
    "    output_file.write(\"\\n\")\n",
    "    \n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f2a864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "criterions = [nn.CrossEntropyLoss(), nn.MSELoss()]\n",
    "optimizers = [optim.SGD, optim.Adam]\n",
    "activations = [F.relu, F.sigmoid]\n",
    "learning_rates = [0.01]\n",
    "epochs = [ 20, 50, 100, 200]\n",
    "batch_size = [ 32, 64, 128]\n",
    "schedulers = [optim.lr_scheduler.ExponentialLR, optim.lr_scheduler.ReduceLROnPlateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f92ff2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all possible parameter combinations\n",
    "parameter_grid = itertools.product(learning_rates, batch_size, epochs, schedulers, optimizers, activations, criterions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76aafcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in parameter_grid:\n",
    "    learning_rate, batch_size, epochs, scheduler, optimizer, activation, criterion = params \n",
    "    # print all params\n",
    "    # print(params)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Build the CNN model with the given parameters\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 6, 5) \n",
    "        # output size = 6 *44*44 values \n",
    "        # image size : n*n \n",
    "        # filter size: f*f (f is odd number)\n",
    "        # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "        # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "        # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "            \n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #output size = 18 * 18 * 16 = 5184   \n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool(activation(self.conv1(x))) \n",
    "            # 44*44*6 , 22*22*6 \n",
    "            \n",
    "            x = self.pool(activation(self.conv2(x)))\n",
    "            # 18*18*16 , 9*9*16 \n",
    "            \n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = activation(self.fc1(x))\n",
    "            x = activation(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    train_and_validate(epochs, optimizer, scheduler , criterion, model, trainloader, validloader, batch_size, learning_rate)\n",
    "    plt.savefig(f\"plots/loss_plot_{params}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
