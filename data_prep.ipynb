{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6176ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "plt.ion() \n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfff8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 6 #how many units in each insert in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22df4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe04620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d8c1799-178e-400a-b147-0638c2f2c801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample #return a transformed image with label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82970810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     class to transform to a normalized tensor (only the image pixel value is transformed)\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotions = sample['image'], sample['emotions']\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        return {'image': transform(image),\n",
    "                'emotions': emotions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794873de-0f5b-41a4-8314-e73ce093e3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(os.path.join(train_folder_path,\"label.csv\"), train_folder_path, transform=ToTensor())\n",
    "valid_dataset = FERPlusDataset(os.path.join(valid_folder_path, \"label.csv\"), valid_folder_path, transform=ToTensor())\n",
    "test_dataset = FERPlusDataset(os.path.join(test_folder_path, \"label.csv\"), test_folder_path, transform=ToTensor())\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f39aca-410d-4b30-b389-d2cdcc8d9170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([6, 1, 48, 48]) torch.Size([6, 10])\n",
      "1 torch.Size([6, 1, 48, 48]) torch.Size([6, 10])\n",
      "2 torch.Size([6, 1, 48, 48]) torch.Size([6, 10])\n",
      "3 torch.Size([6, 1, 48, 48]) torch.Size([6, 10])\n",
      "{'image': tensor([[[[0.0078, 0.0000, 0.0078,  ..., 0.9882, 0.9922, 0.9922],\n",
      "          [0.0039, 0.0039, 0.0078,  ..., 0.9765, 0.9804, 0.9843],\n",
      "          [0.0000, 0.0000, 0.0039,  ..., 0.9373, 0.9725, 0.9922],\n",
      "          ...,\n",
      "          [0.3451, 0.3451, 0.3569,  ..., 0.9804, 0.9686, 0.9569],\n",
      "          [0.3647, 0.3961, 0.3608,  ..., 0.9804, 0.9725, 0.9608],\n",
      "          [0.3765, 0.3843, 0.3725,  ..., 0.9804, 0.9725, 0.9608]]],\n",
      "\n",
      "\n",
      "        [[[0.7137, 0.6824, 0.4549,  ..., 0.1922, 0.0588, 0.1333],\n",
      "          [0.7333, 0.6902, 0.4235,  ..., 0.1569, 0.0078, 0.1176],\n",
      "          [0.7569, 0.6824, 0.4706,  ..., 0.1412, 0.0196, 0.0667],\n",
      "          ...,\n",
      "          [0.5608, 0.4863, 0.6784,  ..., 0.0039, 0.0471, 0.0510],\n",
      "          [0.5569, 0.4627, 0.6000,  ..., 0.0588, 0.0745, 0.0392],\n",
      "          [0.6196, 0.4471, 0.5451,  ..., 0.0863, 0.0941, 0.0745]]],\n",
      "\n",
      "\n",
      "        [[[0.7294, 0.8510, 0.5294,  ..., 0.6235, 0.6314, 0.6275],\n",
      "          [0.7294, 0.5216, 0.2941,  ..., 0.6235, 0.6275, 0.6471],\n",
      "          [0.4667, 0.3922, 0.4275,  ..., 0.6118, 0.6275, 0.5569],\n",
      "          ...,\n",
      "          [0.6431, 0.7333, 0.3333,  ..., 0.0784, 0.5176, 0.4314],\n",
      "          [0.5804, 0.5451, 0.2863,  ..., 0.0078, 0.0275, 0.4588],\n",
      "          [0.4000, 0.4235, 0.0980,  ..., 0.0235, 0.4157, 0.7451]]],\n",
      "\n",
      "\n",
      "        [[[0.6588, 0.5373, 0.5686,  ..., 0.3843, 0.3176, 0.3216],\n",
      "          [0.5804, 0.5529, 0.6196,  ..., 0.3922, 0.3373, 0.3137],\n",
      "          [0.6392, 0.6039, 0.5961,  ..., 0.4039, 0.3569, 0.3216],\n",
      "          ...,\n",
      "          [0.9255, 0.9294, 0.9333,  ..., 0.9294, 0.9373, 0.9412],\n",
      "          [0.9294, 0.9333, 0.9294,  ..., 0.9373, 0.9412, 0.9373],\n",
      "          [0.9294, 0.9294, 0.9294,  ..., 0.9412, 0.9412, 0.9412]]],\n",
      "\n",
      "\n",
      "        [[[0.7569, 0.7765, 0.7216,  ..., 0.3922, 0.3569, 0.3255],\n",
      "          [0.7569, 0.7647, 0.6431,  ..., 0.4706, 0.3608, 0.3333],\n",
      "          [0.7529, 0.7569, 0.5176,  ..., 0.4941, 0.3882, 0.3569],\n",
      "          ...,\n",
      "          [0.1843, 0.3333, 0.4196,  ..., 0.3333, 0.3294, 0.2745],\n",
      "          [0.2078, 0.3373, 0.4157,  ..., 0.3333, 0.3255, 0.2745],\n",
      "          [0.3294, 0.3725, 0.4118,  ..., 0.3373, 0.3294, 0.2784]]],\n",
      "\n",
      "\n",
      "        [[[0.8118, 0.8353, 0.8588,  ..., 0.2627, 0.2431, 0.3216],\n",
      "          [0.8235, 0.8431, 0.8431,  ..., 0.2118, 0.4000, 0.5804],\n",
      "          [0.7569, 0.7373, 0.7098,  ..., 0.2353, 0.5529, 0.6353],\n",
      "          ...,\n",
      "          [0.6431, 0.6863, 0.6980,  ..., 0.8314, 0.8314, 0.8314],\n",
      "          [0.6667, 0.6431, 0.6667,  ..., 0.8510, 0.8471, 0.8314],\n",
      "          [0.7137, 0.6392, 0.6275,  ..., 0.8431, 0.8549, 0.8392]]]]), 'emotions': tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 3.,  0.,  0.,  3.,  2.,  0.,  0.,  0.,  2.,  0.],\n",
      "        [ 0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  8.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
      "        [ 0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  9.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]])}\n"
     ]
    }
   ],
   "source": [
    "# JUST FOR DEBUGGING \n",
    "np_img = train_dataset[3]['image']\n",
    "plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "for i_batch, sample_batched in enumerate(trainloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['emotions'].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        print(sample_batched)\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5af5b92d",
   "metadata": {},
   "source": [
    "**the MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1536c39d-bb61-4f46-9ff2-e9c15dcfaa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "activation_func = F.relu\n",
    "# activation_func = F.sigmoid\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) \n",
    "    # output size = 6 *44*44 values \n",
    "    # image size : n*n \n",
    "    # filter size: f*f (f is odd number)\n",
    "    # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "    # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "     #output size = 18 * 18 * 16 = 5184   \n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(activation_func(self.conv1(x))) \n",
    "        # 44*44*6 , 22*22*6 \n",
    "        \n",
    "        x = self.pool(activation_func(self.conv2(x)))\n",
    "        # 18*18*16 , 9*9*16 \n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = activation_func(self.fc1(x))\n",
    "        x = activation_func(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e0ec98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.0006, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13685b1a",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58e50abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 4.064\n",
      "[1,  4000] loss: 3.227\n",
      "[2,  2000] loss: 2.789\n",
      "[2,  4000] loss: 2.653\n",
      "[3,  2000] loss: 2.404\n",
      "[3,  4000] loss: 2.381\n",
      "[4,  2000] loss: 2.192\n",
      "[4,  4000] loss: 2.190\n",
      "[5,  2000] loss: 2.002\n",
      "[5,  4000] loss: 2.067\n",
      "[6,  2000] loss: 1.872\n",
      "[6,  4000] loss: 1.945\n",
      "[7,  2000] loss: 1.766\n",
      "[7,  4000] loss: 1.852\n",
      "[8,  2000] loss: 1.669\n",
      "[8,  4000] loss: 1.712\n",
      "[9,  2000] loss: 1.610\n",
      "[9,  4000] loss: 1.606\n",
      "[10,  2000] loss: 1.480\n",
      "[10,  4000] loss: 1.561\n",
      "[11,  2000] loss: 1.392\n",
      "[11,  4000] loss: 1.497\n",
      "[12,  2000] loss: 1.342\n",
      "[12,  4000] loss: 1.411\n",
      "[13,  2000] loss: 1.302\n",
      "[13,  4000] loss: 1.347\n",
      "[14,  2000] loss: 1.225\n",
      "[14,  4000] loss: 1.303\n",
      "[15,  2000] loss: 1.211\n",
      "[15,  4000] loss: 1.231\n",
      "[16,  2000] loss: 1.138\n",
      "[16,  4000] loss: 1.213\n",
      "[17,  2000] loss: 1.107\n",
      "[17,  4000] loss: 1.181\n",
      "[18,  2000] loss: 1.081\n",
      "[18,  4000] loss: 1.133\n",
      "[19,  2000] loss: 1.051\n",
      "[19,  4000] loss: 1.094\n",
      "[20,  2000] loss: 1.010\n",
      "[20,  4000] loss: 1.064\n",
      "Execution time: 00:08:47\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        labels = data['emotions']\n",
    "        inputs = data['image']\n",
    "        # print(\"labels\" , labels)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        # print(\"loss\", loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "elapsed_time = time.time() - st\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb11c07f",
   "metadata": {},
   "source": [
    "**Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680e743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 65 %\n",
      "CPU times: user 1.56 s, sys: 759 ms, total: 2.32 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        labels = data['emotions']\n",
    "\n",
    "        images = data['image']\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, labels = torch.max(labels, 1)\n",
    "#         print(\"labels after max: \", labels)\n",
    "        \n",
    "#         print (type(labels) , \" \" , labels)    \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "#         print(labels.size(0))\n",
    "#         print(\"prediction shape: \", predicted.shape , \"label shape: \" , labels.shape)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb4e51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptimization parameters : \\nepochs, batch size, lr (scheduler), optimizer, loss function, activation function, arch. \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "optimization parameters : \n",
    "epochs, batch size, lr (scheduler), optimizer, loss function, activation function, arch. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96e9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b330bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
