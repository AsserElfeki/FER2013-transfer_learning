{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.mps \n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "# plt.ion() \n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample #return a transformed image with label\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     class to transform to a normalized tensor (only the image pixel value is transformed)\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotions = sample['image'], sample['emotions']\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        return {'image': transform(image),\n",
    "                'emotions': emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(os.path.join(train_folder_path,\"label.csv\"), train_folder_path, transform=ToTensor())\n",
    "valid_dataset = FERPlusDataset(os.path.join(valid_folder_path, \"label.csv\"), valid_folder_path, transform=ToTensor())\n",
    "test_dataset = FERPlusDataset(os.path.join(test_folder_path, \"label.csv\"), test_folder_path, transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, scheduler ,criterion, model, trainloader, validloader, learning_rate,):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []\n",
    "    opt_name = optimizer.__name__\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if scheduler == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = scheduler(optimizer)\n",
    "        # print(\"plateu\")\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    elif scheduler == optim.lr_scheduler.ExponentialLR: \n",
    "        scheduler = scheduler(optimizer, gamma=0.9)\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    st = time.time()\n",
    "\n",
    "# Training - Validation loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        model.train()\n",
    "        # Perform training\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # labels = data['emotions']\n",
    "            # inputs = data['image']\n",
    "            labels = data['emotions'].to(device)\n",
    "            inputs = data['image'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # print(\"label before: \", labels)\n",
    "            # print(\"predicted before: \", outputs)\n",
    "            # Calculate and store training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # for performance, you can max in data prep for consistency, \n",
    "            # print(\"label: \", labels)\n",
    "            # print(\"predicted: \", predicted)\n",
    "            # print(\"pred size: \" , predicted.shape)\n",
    "            total += labels.size(0)\n",
    "            correct +=  (predicted == labels).sum().item()\n",
    "            # print(\"1 more correct..\")\n",
    "        \n",
    "              \n",
    "        scheduler.step()\n",
    "                \n",
    "        train_loss.append(running_loss / len(trainloader))\n",
    "        train_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Perform validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for data in validloader:\n",
    "                # labels = data['emotions']\n",
    "                # images = data['image']\n",
    "                labels = data['emotions'].to(device)\n",
    "                images = data['image'].to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                # print(\"total: \" , total)\n",
    "                correct +=  (predicted == labels).sum().item() #can be torch.eq(pred, labels).sum().item()\n",
    "        \n",
    "        valid_loss.append(running_loss / len(validloader))\n",
    "        valid_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Print the training and validation loss and accuracy\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {train_loss[-1]:.4f} | Training Accuracy: {train_accuracy[-1]:.2f}%')\n",
    "        print(f'Validation Loss: {valid_loss[-1]:.4f} | Validation Accuracy: {valid_accuracy[-1]:.2f}%')\n",
    "        print('-----------------------------------')\n",
    "\n",
    "    # elapsed_time = time.time() - st\n",
    "    # print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "    print('Finished Training')\n",
    "    # with open(f'{outputs_path}/statistics.csv', 'a', newline='') as csvfile:\n",
    "    #     writer = csv.writer(csvfile)\n",
    "\n",
    "    #     if csvfile.tell() == 0:\n",
    "    #         # Write the column headers\n",
    "    #         writer.writerow(['trial', 'Batch size', 'Epochs', 'Activation function', 'Loss function', 'Initial Learning rate', 'Optimizer', 'Scheduler',\n",
    "    #                         'Min training accuracy', 'Max training accuracy', 'Average training accuracy',\n",
    "    #                         'Min validation accuracy', 'Max validation accuracy', 'Average validation accuracy',\n",
    "    #                         'Total time', 'device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD\n",
    "activation = F.relu\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "scheduler = optim.lr_scheduler.ExponentialLR\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self, drop=0.2):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) \n",
    "        # output size = 6 *44*44 values \n",
    "        # image size : n*n \n",
    "        # filter size: f*f (f is odd number)\n",
    "        # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "            self.bn1 = nn.BatchNorm2d(6)  # Batch normalization after conv1\n",
    "            \n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "        # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "        # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "            \n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #output size = 18 * 18 * 16 = 5184   \n",
    "            \n",
    "            self.bn2 = nn.BatchNorm2d(16)  # Batch normalization after conv2\n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "            \n",
    "            self.dropout = nn.Dropout(p=drop)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.pool(activation(self.bn1(self.conv1(x)))) \n",
    "            # 44*44*6 , 22*22*6 \n",
    "            \n",
    "            x = self.pool(activation(self.bn2(self.conv2(x))))\n",
    "            # 18*18*16 , 9*9*16 \n",
    "            \n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = activation(self.dropout(self.fc1(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = activation(self.dropout(self.fc2(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42) #do I need it ? wasn't in the current stats\n",
    "model = Net()\n",
    "# model.load_state_dict()\n",
    "model.to(device)\n",
    "\n",
    "print(model.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_and_validate(epochs, optimizer, scheduler , criterion, model, trainloader, validloader,learning_rate)\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(epochs, optimizer, scheduler, criterion, model, trainloader, validloader, learning_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     27\u001b[0m \u001b[39m# Perform training\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[39m# labels = data['emotions']\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39m# inputs = data['image']\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39memotions\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m     inputs \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mFERPlusDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m#   to create the image name\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         img_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_frame\u001b[39m.\u001b[39miloc[idx, \u001b[39m0\u001b[39m])\n\u001b[0;32m---> 27\u001b[0m         image \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mimread(img_name)\n\u001b[1;32m     28\u001b[0m         emotions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_frame\u001b[39m.\u001b[39miloc[idx, \u001b[39m2\u001b[39m:]\n\u001b[1;32m     29\u001b[0m         emotions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(emotions)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/skimage/io/_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[0;32m---> 53\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39;49m\u001b[39mimread\u001b[39;49m\u001b[39m'\u001b[39;49m, fname, plugin\u001b[39m=\u001b[39;49mplugin, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mplugin_args)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/skimage/io/manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    205\u001b[0m                            (plugin, kind))\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/skimage/io/_plugins/imageio_plugin.py:15\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@wraps\u001b[39m(imageio_imread)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(imageio_imread(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/imageio/v2.py:348\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m imopen_args \u001b[39m=\u001b[39m decypher_format_arg(\u001b[39mformat\u001b[39m)\n\u001b[1;32m    346\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39;49m\u001b[39mri\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mimopen_args) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m    349\u001b[0m     result \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread(index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/imageio/core/imopen.py:196\u001b[0m, in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     plugin_instance \u001b[39m=\u001b[39m candidate_plugin(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    197\u001b[0m \u001b[39mexcept\u001b[39;00m InitializationError:\n\u001b[1;32m    198\u001b[0m     \u001b[39m# file extension doesn't match file type\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/imageio/plugins/pillow.py:80\u001b[0m, in \u001b[0;36mPillowPlugin.__init__\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m request\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mio_mode \u001b[39m==\u001b[39m IOMode\u001b[39m.\u001b[39mread:\n\u001b[1;32m     79\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[39mwith\u001b[39;00m Image\u001b[39m.\u001b[39;49mopen(request\u001b[39m.\u001b[39;49mget_file()):\n\u001b[1;32m     81\u001b[0m             \u001b[39m# Check if it is generally possible to read the image.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m             \u001b[39m# This will not read any data and merely try to find a\u001b[39;00m\n\u001b[1;32m     83\u001b[0m             \u001b[39m# compatible pillow plugin (ref: the pillow docs).\u001b[39;00m\n\u001b[1;32m     84\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[39mexcept\u001b[39;00m UnidentifiedImageError:\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     fp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fp\u001b[39m.\u001b[39mread())\n\u001b[1;32m   3234\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 3236\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(\u001b[39m16\u001b[39;49m)\n\u001b[1;32m   3238\u001b[0m preinit()\n\u001b[1;32m   3240\u001b[0m accept_warnings \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_validate(epochs, optimizer, scheduler , criterion, model, trainloader, validloader,learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './mymodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for classess names in FERplus dataset\n",
    "classes = {\n",
    "    0: 'Neutral',\n",
    "    1: 'Happinnes',\n",
    "    2: 'Surprise',\n",
    "    3: 'Sadness',\n",
    "    4: 'Anger',\n",
    "    5: 'Disgust',\n",
    "    6: 'Fear',\n",
    "    7: 'Contempt',\n",
    "    8: 'Unknown',\n",
    "    9: 'NF'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGeCAYAAAA9hL66AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwH0lEQVR4nO3de2yW93n/8ctAefA5mIONOaSmNbCMgzhkiDUrtCneWJSlS/+YlqpKd9CaQqKgqEpL+aPetOEEaYhOtNmyRVmkidE/1nSVuqRYWmMSoWSGwCBkzdKEgDk4DmB8wtjBvn9/5IeLwff18fN88b6P4f2S/Eeey9/78L3v57ny4Ou6vwVJkiQGAEAEE2IfAADg9kUSAgBEQxICAERDEgIAREMSAgBEQxICAERDEgIAREMSAgBEQxICAEQzKfYBXG9wcNDOnDljpaWlVlBQEPtwAABZSpLEurq6rLq62iZMEN91kjHygx/8IPn0pz+dZDKZZMWKFcm+fftGNa6lpSUxM3744Ycffsb5T0tLi/zMH5NvQj/60Y9s8+bN9sMf/tA+97nP2T/8wz/Yhg0b7O2337Z58+a5Y0tLS83M7K//+q9typQpI/6Ol1lV1k3Eo/JUfOLEiW7co46tq6srNdbb2+uOPXLkiBt/7733UmPnzp1zx6p9f/zxx258YGAgNbZkyRJ37Le+9S03fvV+GcnkyZPdsWn311WZTMaNe/eC+hb/qU99yo2r+zBkrHc91PiQsWZmfX19OW/7jTfecOM/+9nPUmOnT5/O+bjM9D3uxdV9pO7T/v7+nMdeunTJjav7dNKk9BRx+fLl1FiSJHbx4kX3/Tm0D/kbOdixY4f92Z/9mf35n/+5mZnt3LnTfv7zn9szzzxjDQ0N7tirkzJlyhQrLCwc8Xdu1STk3cjquNSHmnfc6rjUjRoS925yM7OioiI3XlxcnBpTb9C0++uqkCSk5jSfk9Dg4GDOY0PuU7Vt9T8N3r2krsdYxvN52+q9m+u+r95Do/mTyk0vTOjv77eDBw9aXV3dsNfr6ups//79N/x+X1+fdXZ2DvsBANwebnoSOnfunA0MDFhlZeWw1ysrK621tfWG329oaLDy8vKhn7lz597sQwIA5KkxK9G+/mtYkiQjfjXbsmWLdXR0DP20tLSM1SEBAPLMTf+b0PTp023ixIk3fOtpa2u74duR2Sf/7q7+7R0AcGu66Ulo8uTJtnLlSmtsbLQ//MM/HHq9sbHRHnjggVFvZ3Bw0P0DaRr1hzD1x0/F+2Nc6L69P+qqCrWenh437sllnq+lijW8+Be+8AV3bFlZmRv35nys+8y86yl7IwTvXggtFAkpelDX+sqVK27cK0xQFWiqsnbhwoWpMfW3Zq8y1Uy/d715URVqqoDG27eab7Vt9bni3Ssh1aHXGpPquCeeeMK+9rWv2apVq2zNmjX27LPP2smTJ+2RRx4Zi90BAMapMUlCf/RHf2Tnz5+3v/qrv7KzZ8/a4sWL7T/+4z/szjvvHIvdAQDGqTF7bM/GjRtt48aNY7V5AMAtgAeYAgCiIQkBAKIhCQEAosm7pRyuSpIktTzQKylWJaSqdFaVK4eUznoPIlTjOzo63LGK97BBNWfqOWfd3d1ufNmyZamxFStWuGNVaaz3/Dd1XqGlzCHPEVRCHiIaWh7ubV8960+VBHtl2Oo+q6iocOO/+Zu/mRp799133bGq1Fm1QHjj1X2myqS9zyQ1Vj0fMeQ5gzfr/uebEAAgGpIQACAakhAAIBqSEAAgGpIQACAakhAAIBqSEAAgmrztE8qVqntX/QChPRYedWxezb96HLw6L69PSPWdhC5/cc8996TGysvL3bGqF8Hrg1A9LaovJWSJCrVv1Y8W0oOhlkRQ97gXV9tW5+XNeVFRkTtW9erMmjUrNTZ79mx3rPf+MNNLPXjvXdUnpObUu5fUe7Ovr8+Nq/dArn2R2SzlwDchAEA0JCEAQDQkIQBANCQhAEA0JCEAQDQkIQBANCQhAEA047JPyKuNVz0Qqn9DUb0+HlU7r3qBPCH9Uaq3Q/VnqB6MJUuWpMZUP4xan8a7nplMxh2reiRC7hV1H2bTR3G9sV4nyTvvkJ4WM/8+VMelrqe37alTp7pj1X2m3iPeeasePjWn3n2qrrXat1pvyLuPveul5mvYPkb9mwAA3GQkIQBANCQhAEA0JCEAQDQkIQBANCQhAEA0JCEAQDR52ydUUFCQWgPv1a5nU58+kpDeELVv1TvS39+f87a7u7vduOpF8Kheg4ULF7pxb50X1adQXFzsxr0eCtX7MWXKFDeu+la8fatrra6nFw9dL0idl9eHFLIWkYp7a/KY6Xtl2rRpOR9XaWmpG1f3infs6j4M6RlT2w5535v594IXo08IADAukIQAANGQhAAA0ZCEAADRkIQAANGQhAAA0eRtibYnpKQxpDTWLKwsUcW981IlppcvX3bjXum5GqtKeleuXOnGvfLXkpISd6wqjfXOSy15EPIIfUWVtYeU86t7QR23Ou+QNoe+vr6c9+0t0TIa3r1UUVHhjn377bfdeEiZtbpeRUVFbty7HmrOVMtJyL0wZ84c97g++OADd9tX8U0IABANSQgAEA1JCAAQDUkIABANSQgAEA1JCAAQDUkIABBN3vYJLV68OPUx/l7t+8WLF93tnjp1KuSw3J4Z1V+heke8mv2Q/gu170uXLrljFyxY4MbvvvtuN+71SaheHm95CzP/eqhth85pSL+a4vWlhPbTqD6ikOUW1Jx5x67eH6pfzTtu1SeklnJobW1145lMJjUWMt9m/n2q+oDUe0Dx9u31ZalreS2+CQEAoiEJAQCiIQkBAKIhCQEAoiEJAQCiIQkBAKIhCQEAosnbPqH58+en1u57tfGqj6GystKNqz4HL67W5VE9St7aOeq8VM+LV7dfVlbmjl2/fr0bV70KnpA+IDO/N0v18ah9K955q32rnjLveqptq3tF9dKdO3cu57EtLS1u3JtzNSdqbSmvb0Udt+oTUnHvc0G9P0LWrVIKCwvduDq27u7unGLZ9LLxTQgAEA1JCAAQDUkIABANSQgAEA1JCAAQDUkIABBN3pZoDw4OppZseuXG3iPVzcxmzpwp9+vxSg9VWaJ6vLlX3hq6TIQ3XpVo//d//7cb7+rqcuNf+cpXUmNz5sxxx6o59cqVQ+dMPWLfG6/KqNV5efvu7Ox0x77xxhtu/MCBA278/PnzqTFVJq1KmcvLy1NjU6dOdceqFghvXlQ5vroXVGuHV66seKXlZv69pI5blWB7S4aY+dfbK0tXrRXX4psQACAakhAAIBqSEAAgGpIQACAakhAAIBqSEAAgGpIQACCavO0T8ng9FKG9IYrX36F6Q1Tco3okVF2+15+xaNEid6z3aH8zs//6r/9y416vgddDpMaa+X0OEydOdMeqZTvU+JAlLEL60Y4cOeKO/eijj9y4erz/woULU2OLFy92x86bN8+Ne/eh6jFS18Pz4YcfunHVW9Xc3OzGa2pqUmPqeqilHLz+KdWjpz7vVNy7V7zlRtT9fa2svwnt27fP7r//fquurraCggL7yU9+MiyeJInV19dbdXW1FRYW2rp16+zYsWPZ7gYAcBvIOgn19PTYsmXLbNeuXSPGt2/fbjt27LBdu3ZZc3OzVVVV2fr162XGBgDcfrL+94QNGzbYhg0bRowlSWI7d+60rVu32oMPPmhmZi+88IJVVlba7t277Rvf+EbY0QIAbik3tTDh+PHj1traanV1dUOvZTIZW7t2re3fv3/EMX19fdbZ2TnsBwBwe7ipSai1tdXMbnzYX2Vl5VDseg0NDVZeXj70M3fu3Jt5SACAPDYmJdrXV4ElSZJaGbZlyxbr6OgY+vGeJA0AuLXc1BLtqqoqM/vkG9GsWbOGXm9ra0t9FHomk5HLLwAAbk03NQnV1NRYVVWVNTY22vLly83sk3U8mpqa7Omnn75p+/Fq0LOpTx+J6rfJZp2MbMd6vSGXLl1yx6oepNmzZ6fGvv71r7tje3t73fhbb73lxk+ePJnz2N/4jd9w4946MSF9JWZ6TkP61dT6Nt7fRlV/ktezYmb2mc98xo17/ySu1r5RazB5cbUmT3FxsRsvKipKjak5Udfjl7/8pRv3embU2lGnT592417/lOofVPeKd9xm+nreDFknoe7ubvvVr3419N/Hjx+3w4cPW0VFhc2bN882b95s27Zts9raWqutrbVt27ZZUVGRPfTQQzf1wAEA41/WSejAgQP2hS98Yei/n3jiCTMze/jhh+2f//mf7cknn7Te3l7buHGjtbe32+rVq23v3r2yGxoAcPvJOgmtW7fO/WelgoICq6+vt/r6+pDjAgDcBniAKQAgGpIQACAakhAAIJpxuZRDSBl2yHIKZn65pSoJVqWaXrlkSHm3mV92O336dHesokpnvfJwVZZ75swZN15RUZEaU+XE6hH66nqG3IchY9VyCYoqu/XKftWSCIp33up6qXJjteyHx1sSZDT+93//NzWmirLUZ1J7e3tqLKRlxEyXaHvH5t0n6vPoWnwTAgBEQxICAERDEgIAREMSAgBEQxICAERDEgIAREMSAgBEMy77hEKELvUQ8mhzVdPv1d2rR80rXq9CR0eHO/bChQtuXPVvhMx5W1ubG+/p6UmNqf6M8vLynI7pKq/PSC1/EbJt1YOh5lv1hnjX25tvM73kyMcff5waU+8ttRzDnXfemRqbNm2aO1b1KHnHbeYvxzBjxgx3rOpvOnfuXGpMvfdUD1LIUhDe51k2/Ut8EwIAREMSAgBEQxICAERDEgIAREMSAgBEQxICAERDEgIARJO3fUJJkqTWmnu176o+XfVQqF4Fb9+h/RshvSWZTCbnbb/22mvu2DfffNONqx6LysrK1FhZWZk7VvXytLa2jtm21XpD3pyr/gzV9+XdS+o+Ub08qjfEW8Pp4sWL7ljVm1VUVJTzto8cOeLGvR6l2tpad6y61t66VYo6L29OzPzrre4z1ROmPrOuXLmS07az6Q3kmxAAIBqSEAAgGpIQACAakhAAIBqSEAAgGpIQACAakhAAIJq87RPyeLXtqk9Ixb26eLVvVXOvepC8Y1PHpbb9/vvvp8beeustd+zZs2fd+Ny5c9341KlTU2PHjh1zx6oeCm+NGW8dFjPdG6L2PXny5NSY6uVR19Nbv0b1nbS3t7vxrq4uN/7GG2+kxlpaWtyxam2c4uLi1NicOXPcsdOnT3fj3j2u5nvRokVuvKqqyo17/VEffvihOzZkTSDV86X60VSf0fnz53MaS58QAGBcIAkBAKIhCQEAoiEJAQCiIQkBAKIhCQEAosnbEu2BgYHUkmdVVhhClXB7VFliyDITIUtMmJm1tbWlxlSJ6MKFC934t771LTdeWFiYGtu6das79r333nPj3rIF1dXV7livdNxMl2h7Swd4MTN/Tsz8e6Gjo8Mdq0qCjx496sa98vJ77rnHHTtt2jQ3/vOf/zw1tnv3bnesagVYvnx5akzNmSotnzlzphv3litR10OVj3v3QmjrhnrveyXgXotDNp+jfBMCAERDEgIAREMSAgBEQxICAERDEgIAREMSAgBEQxICAESTt31CuZo4caIbz+YR49lSSzmomn71WHaP6hPylh1Q/TSql0AtHXD69OnUmOpjmDVrlhs/efJkakz1+ZSUlLhxrwfJzO89UfeC9+h/M//Y1XmppRpmzJjhxtevX58aW7p0qTtW9Ql5/Wpq+QsV/9nPfpYaW7FihTtW9QmtWrXKjc+fPz819j//8z/uWHVe3vtPve+9JUHM9PtPfWaloU8IADAukIQAANGQhAAA0ZCEAADRkIQAANGQhAAA0ZCEAADR3HJ9Qqo/o7u7242runqvD0ntW23b62Hy+nzUcZn5xxa6Row6b68H4ytf+Yo7VvVOeev2rF692h2rrofqt/F6KNT1Uv0X/f39qbHy8nJ3rFp3R/WlvPvuu6kx1RNWVVXlxr2eF299GjPdy+Nt+8yZM+7YlpYWN6566RYtWpQae/XVV92x586dc+NeX5i6z/r6+oLi3meS1wtEnxAAYFwgCQEAoiEJAQCiIQkBAKIhCQEAoiEJAQCiydsS7SRJUsv8vMeXq9JAVaKtlnrIZDJu3KNKgkNKHtW2FyxYkFPMTJ9zRUWFG/fKkVVpeWFhoRv/zGc+kxqbPXu2O1Yt1eCVf5vpx+B7VIl2yJxNnTrVjavr6ZXcq5LeEydOuHGvfLympsYdq5YtOH/+fGpMve/VnKrr5S2PoZaReOmll9y4995W96D6PFNz6n3ueG0ElGgDAMYFkhAAIBqSEAAgGpIQACAakhAAIBqSEAAgGpIQACCavO0T8oT0Z3iPRTfT/QDe4+JVr44Ssm11Xn/wB3+QGlN9JR0dHW78woULbtzrVVDHrfqEvGUNVE+L6gNSvSXesalH7CvevMyaNcsdq5agUPPiLZlwxx13uGPV9fL6UtSchSyVos5ZUX1E3jIUv/u7v+uOff/99934O++8kxoLXeJFfa5421c9SKOV1ad5Q0OD3X333VZaWmozZ860L3/5yzdMUJIkVl9fb9XV1VZYWGjr1q2zY8eO3ZSDBQDcWrJKQk1NTbZp0yZ7/fXXrbGx0a5cuWJ1dXXDOs+3b99uO3bssF27dllzc7NVVVXZ+vXr5f+dAQBuP1n9c9zLL7887L+ff/55mzlzph08eNA+//nPW5IktnPnTtu6das9+OCDZmb2wgsvWGVlpe3evdu+8Y1v3LwjBwCMe0GFCVf/VnD12WHHjx+31tZWq6urG/qdTCZja9eutf3794+4jb6+Puvs7Bz2AwC4PeSchJIksSeeeMLuueceW7x4sZmZtba2mplZZWXlsN+trKwcil2voaHBysvLh368BxwCAG4tOSehRx991I4cOWL/+q//ekPs+gqYJElSq2K2bNliHR0dQz8tLS25HhIAYJzJqUT7scces5/+9Ke2b98+mzNnztDrVVVVZvbJN6Jry0jb2tpu+HZ0VSaTCVoeAQAwfmWVhJIksccee8xefPFFe+WVV25Y/6OmpsaqqqqssbHRli9fbmafrDnR1NRkTz/99E07aNXL41HrXKjad69XIaR/yczvNbh8+bI7VvUirF69OjWm/g6nenlCehXUnJWVlbnxkpKS1Jg6L3Wti4uL3bhH3WfqvL2eMdXXtWTJEjeu+rra2tpSY6q3SvGup+pp8eZEjVf3qPfeG03c+1y466673LF/8Rd/4cb/9m//NjX20UcfuWPV+0fdC14fkTenSZKMum8yqyS0adMm2717t/37v/+7lZaWDv2dp7y83AoLC62goMA2b95s27Zts9raWqutrbVt27ZZUVGRPfTQQ9nsCgBwG8gqCT3zzDNmZrZu3bphrz///PP29a9/3czMnnzySevt7bWNGzdae3u7rV692vbu3WulpaU35YABALeOrP85TikoKLD6+nqrr6/P9ZgAALcJHmAKAIiGJAQAiIYkBACIhiQEAIgmb9cTunLlSmqduddjoXoJVO16yForqvdD9Td5hR9Xn8+X5trn9WU7fvr06e7Y0PPyeijU09VVVWVIz5jqS1F9QiFr1Kg59fpSVIFQaF/XjBkzUmOqT+jaJ+qPpLe3NzWmenHUWkUe9bngrXM0mvEedY9+7nOfc+Peel47d+50x6rz8tbjMvP7iPr7+1Njoyliu4pvQgCAaEhCAIBoSEIAgGhIQgCAaEhCAIBoSEIAgGjytkS7oKAgtYw1ZMkEr8TaTD/e3yt5DH0Uvbft65fNuJ56vL+3FISak9DH3Hd3d6fGVAm2KjEd7ePiR+KVjpuF3QuhS4Z449U5h5R/m/n3qRqrlg4IKWtXQpYMUe/dkPHqPlMl3L/3e7/nxj3PPfecG1f3krdUive+zgbfhAAA0ZCEAADRkIQAANGQhAAA0ZCEAADRkIQAANGQhAAA0eRtn9DEiRNl7f5IVH9GaI9FSC+C4h17bW2tO1bNlderoPoYQns7vDmfOXOmO1YtHRDSTxOyDISZf73V9VD9Nl4fkbrPVD9aJpNx4yH3sZpTbxmJXN7vo6XmO5ulB0biHbt6/6ieMW/bqodo3rx5bvzZZ59142+99VZqzFu2g6UcAADjAkkIABANSQgAEA1JCAAQDUkIABANSQgAEA1JCAAQTd72CSVJklprrtaY8YSu+eMJXbPE66GYPn160L69bau+EaW9vd2Nl5eXp8bUcau+k8LCwtSY6r/o7+934+o+8469qKjIHRuyrlXInJjpOQ/pE1Jz5vXrqLEq7r2/QtZQMtO9Pl5Pmtq2+lzw7lPVC7dw4UI3/p3vfMeNv/rqq6mxffv2pcauXLlir732mrvtq/gmBACIhiQEAIiGJAQAiIYkBACIhiQEAIiGJAQAiCZvS7QLCgpyKsVWY1T5qirF9MpX1ZIIIeWrpaWlOR+XmV/Wq0p+1XIKat/enKsyaVXe6s2ZKstV+1bnFbKMhCof96iSXrVvNafefRy6jIT3HvDaCEbDux4hrRej4c1L6FIo3pypzxS175KSEjd+3333pcbWrFmTGuvp6aFEGwCQ/0hCAIBoSEIAgGhIQgCAaEhCAIBoSEIAgGhIQgCAaPK2T8jj9TGE9gN4vQZmfj+A6s8IeVS96s9Q+w7R2dnpxlUPU29vb2pMLSPhLQNh5vcwefs1C1/SwOtDUvs+e/asG/fuQ7UMREVFhRtXc+7dhyF9W2q8GquO2+v7Uj18qldO9XV57z91H6ljC+l/Up8L6vPu8uXLqbGQZVSuxTchAEA0JCEAQDQkIQBANCQhAEA0JCEAQDQkIQBANCQhAEA047JPSNXde0J6dRTV56Diap0Yj+oH8OZM9QG1t7e7cXVeXh9EZWVlzmPN/D4h1QOh+pvU+jZej4bq5Tl9+rQb9/pWzp07545VczZt2jQ37p23eu+N5ftLXc8Qas7U+8uLq22HzKnqb1LzrebU6/fxzksd17X4JgQAiIYkBACIhiQEAIiGJAQAiIYkBACIhiQEAIiGJAQAiCZv+4QGBgZS69DHsk8opG5e9ZWofhrvvNT6HCrurbXS19fnjr1w4YIbD1l3p6enxx0bsr7THXfc4Y5VvTxq/Rpvzr3+JTN9H3rbVusFtbS0uHF1n5aUlKTG1Po1Ib08IXOi9q16dbz3h5nuE/KOLZuemZGEzKnqPQz5XMk1dj2+CQEAoiEJAQCiIQkBAKIhCQEAoiEJAQCiIQkBAKLJ2xLtXIWUWI8m7pU8qjLQkGNTJb8qrspyPSdPngyKL1iwIDWm5lstM3H+/PnUmCpPDSlVNvOXglBluR0dHW68rKwsNaaWv1DlxKdOnXLjn/70p1Njak5VWXthYWFqTLUwqPPyWg1C35sq7m1f3eOqNN0ri1fbVu0XY7k8xmhl9U3omWeesaVLl1pZWZmVlZXZmjVr7KWXXhqKJ0li9fX1Vl1dbYWFhbZu3To7duzYTT9oAMCtIaskNGfOHHvqqafswIEDduDAAfviF79oDzzwwFCi2b59u+3YscN27dplzc3NVlVVZevXr7eurq4xOXgAwPiWVRK6//777fd///dtwYIFtmDBAvubv/kbKykpsddff92SJLGdO3fa1q1b7cEHH7TFixfbCy+8YJcuXbLdu3eP1fEDAMaxnAsTBgYGbM+ePdbT02Nr1qyx48ePW2trq9XV1Q39TiaTsbVr19r+/ftTt9PX12ednZ3DfgAAt4esk9DRo0etpKTEMpmMPfLII/biiy/aXXfdZa2trWZ24x9NKysrh2IjaWhosPLy8qGfuXPnZntIAIBxKusktHDhQjt8+LC9/vrr9s1vftMefvhhe/vtt4fi11d6JEniVn9s2bLFOjo6hn7UwxcBALeOrEu0J0+ebJ/97GfNzGzVqlXW3Nxs3//+9+3b3/62mZm1trbarFmzhn6/ra3NLSnNZDKyrBMAcGsK7hNKksT6+vqspqbGqqqqrLGx0ZYvX25mnzwevampyZ5++ungAx0tVXOvqLp71W/gUX0O3rZVvX93d7cbD+nPmD59uhs/dOiQG/d6S7xeGzPdx+D100ydOtUdW1xc7MZ7e3vduPet3ZtvM7Pq6mo3PmPGjNSYWk7B6/MxM3v33XfduPfP5+qfy9V9GnIfqnvB681SSzWovq6QzwX1maQ+F0I+07JZUmEk3jXx7sNslq/IKgl997vftQ0bNtjcuXOtq6vL9uzZY6+88oq9/PLLVlBQYJs3b7Zt27ZZbW2t1dbW2rZt26yoqMgeeuihbHYDALhNZJWEPvzwQ/va175mZ8+etfLyclu6dKm9/PLLtn79ejMze/LJJ623t9c2btxo7e3ttnr1atu7d6/8v10AwO0pqyT03HPPufGCggKrr6+3+vr6kGMCANwmeIApACAakhAAIBqSEAAgGpIQACCacbmeUEjdvBo7YYKfl736d1WTH7ImyYULF9yx06ZNy3nbSlFRkRtfs2aNG/fWzlE9K6ovxbueU6ZMccd6PUZmfq+Omdm8efNSY6oBW82pd6+oa6l6NGbPnu3Gf/WrX6XGzp49645VvPeXev+ofhovrsZevnzZjYesMxa6xlkI1VOm4t6x36w+Ib4JAQCiIQkBAKIhCQEAoiEJAQCiIQkBAKIhCQEAosnbEu0JEybIcumRhJZgh5R/q9JZtW2vzFOVaIcsi67KiVWpc0lJiRu/4447UmNqmQj1eP+Q66VKZ9W2vWUJ1NhsSliv590nZvp6qZJgb5mJkydPumNDjk0dl5ozL66utTpudWwhZdbqMylk2+q8Jk+e7MZzLalX+x22j1H/JgAANxlJCAAQDUkIABANSQgAEA1JCAAQDUkIABANSQgAEE3e9gkNDg6m1qGHPDY9tE8oZN+K92j07u5ud6x6FL13XmpsaB+D1yc0Z84cd6zqE7p06VJqLPR6hNwr6rjVtr1jV/Ot9q0e319aWpoau/POO92xZ86cceN9fX2pMW9pDLOw+1DdC95xmeleupAepZDrGXovhMzpzVqCgm9CAIBoSEIAgGhIQgCAaEhCAIBoSEIAgGhIQgCAaEhCAIBo8rZPyOOt2zOW68+YhfVvKF7/xscff+yOVb0+ixcvTo0dP37cHavWkFG9Pl4PhVp3pLy83I174/v7+3Mea6b7adS9FrLvXNbSukr1vITw1lAy89ciMjNrb29PjZ04ccIdO2vWLDfuzWnIfJqN7fpPak2fkLFqjbOQuDen2cw334QAANGQhAAA0ZCEAADRkIQAANGQhAAA0ZCEAADR5HWJdi6P4g8tkw4t4faElEOq8zp9+rQbX716dWpsyZIl7thjx4658ffffz/nuHpE/uzZs934jBkzUmMdHR3uWHV/qRJsb8mDiooKd6wqdfZKXNVYdQ+ruLfv3t5ed6xqJZg5c2ZqzCvfHs2+S0pKch6r7kNVchxSZq14rQLqc0F95ii5LuWQzecw34QAANGQhAAA0ZCEAADRkIQAANGQhAAA0ZCEAADRkIQAANHkdZ9QWj+D19+hHrmuHquuhPQhqbHeeameFtUT093dnRqbN2+eO/auu+5y4+fOnXPjFy5cSI0VFRW5Yw8fPuzGKysrU2Oqz0fdK6p3ZNq0aamxrq6uoG1794o67tD3gNdv09nZ6Y69ePGiG/eul7oPVc+LupfGklr2wzOWvYljybse2fQn8U0IABANSQgAEA1JCAAQDUkIABANSQgAEA1JCAAQDUkIABBN3vYJTZgwIXUND68GPXQtFdWPk8saR/8X+vr63LjX33H58mV3bHl5uRtfsWKFG9+/f39qrKyszB07ffp0N+5dT3Xcly5dcuNqTvv7+924R92H3vo358+fd8eqvq1Tp065ce/9VVNT445V5+XNueoT8vqy1L6nTJnijlXrIKneqpBen5CeMUWdl+Lt23t/ZPPe4JsQACAakhAAIBqSEAAgGpIQACAakhAAIBqSEAAgGpIQACCavO0TGhwczGpNiqvSeotGK6TPKLTHKJfzvUr1vHh9QqG9BKrPobi4ODU2f/58d2xVVZUb99ZxKSwsdMeqfhu1RpOntLTUjau+lQ8++CA1pnpWVC/PkiVL3Hh1dXVqrKKiwh370UcfuXFvzkPX+vJ6q9TaUiqujs0bP9afSR71maN6kLzPJO9zI5vPFL4JAQCiIQkBAKIhCQEAoiEJAQCiIQkBAKIhCQEAosnbEm2PV3aoSg5VuWRIOaUqhwxZRkKdl3p0emtra2pMlfSqUmd1XiUlJakxVQZ95coVNz558uTUmCq7VVQ5slearu4jdV4LFy7Mab9menkMVW7slfur5S2862FmNmPGjNSYul5qzrw5V+8fr9R/NHGPuhfUsXlx1dah5kzFvWVe8qJEu6GhwQoKCmzz5s1DryVJYvX19VZdXW2FhYW2bt06O3bsWMhuAAC3qJyTUHNzsz377LO2dOnSYa9v377dduzYYbt27bLm5marqqqy9evXW1dXV/DBAgBuLTkloe7ubvvqV79q//iP/2hTp04dej1JEtu5c6dt3brVHnzwQVu8eLG98MILdunSJdu9e/dNO2gAwK0hpyS0adMmu+++++xLX/rSsNePHz9ura2tVldXN/RaJpOxtWvXpi7x3NfXZ52dncN+AAC3h6z/2rZnzx578803rbm5+YbY1T9+V1ZWDnu9srLSTpw4MeL2Ghoa7C//8i+zPQwAwC0gq29CLS0t9vjjj9u//Mu/uA9gvL5aKkmS1AqqLVu2WEdHx9BPS0tLNocEABjHsvomdPDgQWtra7OVK1cOvTYwMGD79u2zXbt22TvvvGNmn3wjmjVr1tDvtLW13fDt6KpMJiNLTgEAt6asktC9995rR48eHfban/zJn9iiRYvs29/+ts2fP9+qqqqssbHRli9fbmaf9K80NTXZ008/fdMOOqSfRvXyqLp5b3zotr3a+pDjMjM7efJkaszrSTHT/TKqN2TZsmWpMbUEhepp8aouVW/HzJkz3bj6n6Py8vLUmFrKIeReUWO9JQ1GE/d6T9S+1b3gzZm6XmrfIX1h6rhV3KPmW523dy+E9kWqPqNce4HU59W1skpCpaWltnjx4mGvFRcX27Rp04Ze37x5s23bts1qa2uttrbWtm3bZkVFRfbQQw9lsysAwG3gpj8x4cknn7Te3l7buHGjtbe32+rVq23v3r3y/wwBALef4CT0yiuvDPvvgoICq6+vt/r6+tBNAwBucTzAFAAQDUkIABANSQgAEA1JCAAQTd6uJ5QkiewLyIWqq1f17d74kLFmYT1Iak2ftra21Nj58+fdsaqyUZ23t57QtU3NI1Hnfccdd6TG1DpI6rzUeK8vRfV+qHvB68FQa/qotVzUsXlrU6njDlnLyFu7xkwft/ceGMt1xMzC+rrU9fJ6edTY7u5uN97T0+PGvXvNuxfUfXItvgkBAKIhCQEAoiEJAQCiIQkBAKIhCQEAoiEJAQCiydsS7YGBAfmY8ZGoUktVyqziY8nbtyrFVEsedHR0pMZ++ctfumNramrcuJozr+TXi5npR+h75d+KKk9V919RUVFqLLRcP+Qx+WpO1fXy3kPFxcXuWG+xSzN/WQM1J+q8vJJ59f5QccWbM/XeDSnRVkuhqLhaZsIT8r6+Ft+EAADRkIQAANGQhAAA0ZCEAADRkIQAANGQhAAA0ZCEAADR5G2fkLeUg9dPEPrY9FOnTuU8XvU5eL06oVRPi3dsr776qjv23nvvdePecgpmfl+K19thpvs31HhPLn1o11L9OiFjQ5ZTUL1Vitf/pPat+lJC5kxda69XJ3RO1OeKd17qPlM9NSHLeqhtq+vpHbt3ztlcZ74JAQCiIQkBAKIhCQEAoiEJAQCiIQkBAKIhCQEAoiEJAQCiyds+ocHBQVnDngvVJ9TS0uLGL1y4kPO+1fo1Ieer+hi8Hov33nvPHXv69Gk3Xl1d7ca9Y1Pr0xQWFrpxr/9DrZujeixUb4nXwxTSD6P2rbatzlv1Xk2alP6xENoH5N3jar694zLz+4TUe0utQ6Z6fXLtpwmNqz6g0PvQu96XL19Ojan31rX4JgQAiIYkBACIhiQEAIiGJAQAiIYkBACIhiQEAIiGJAQAiCZv+4TGiuqRyGQyblz1E4Twtq32q3pDvF6d0L4SdWzenKpthwhddydkraKQtW/MwnrG1LZVPGQto5AeJHU9VC+c16ujjkv106g+Ia8vRm1b9S56vTqhfUBer4+Zfy9cvHgx5+1ei29CAIBoSEIAgGhIQgCAaEhCAIBoSEIAgGhIQgCAaG65Em1VQhpajuyVkarHqqsSU+/Y1HGpuDcvarmEoqIiN64ese+NV+XCas6881b3giqjVuflbV+V9Krr5Z23aiNQ+1Zlvd59rJZyUOfl3WtqrHrveuedzdIC2W7bzJ8zVYKtjs0rd1bHpfatlpfp6OjIKUaJNgBgXCAJAQCiIQkBAKIhCQEAoiEJAQCiIQkBAKLJuxLtq6WpqnQwV6o8tbe31417pYeqRFuVYoaUeaq4d96qlFnNSXd3txv3qBJtVbbrlaiq81Il2GNZoh3yROiQUmWzsBJtdS+EPK16LEu0Q55KrrZt5r+31XtTlTN7n4Wh11pdTy/uHffV+VD3uZlZQTKa3/o/dOrUKZs7d27swwAABGppabE5c+a4v5N3SWhwcNDOnDljpaWlVlBQYJ2dnTZ37lxraWmxsrKy2Ic3LjBn2WPOssecZe92mbMkSayrq8uqq6vlv3bk3T/HTZgwYcTMWVZWdktftLHAnGWPOcsec5a922HOysvLR/V7FCYAAKIhCQEAosn7JJTJZOx73/uefGgjfo05yx5zlj3mLHvM2Y3yrjABAHD7yPtvQgCAWxdJCAAQDUkIABANSQgAEA1JCAAQTd4noR/+8IdWU1NjU6ZMsZUrV9qrr74a+5Dyxr59++z++++36upqKygosJ/85CfD4kmSWH19vVVXV1thYaGtW7fOjh07Fudg80BDQ4PdfffdVlpaajNnzrQvf/nL9s477wz7HebsRs8884wtXbp0qMt/zZo19tJLLw3FmTNfQ0ODFRQU2ObNm4deY85+La+T0I9+9CPbvHmzbd261Q4dOmS/8zu/Yxs2bLCTJ0/GPrS80NPTY8uWLbNdu3aNGN++fbvt2LHDdu3aZc3NzVZVVWXr16+3rq6u/+MjzQ9NTU22adMme/31162xsdGuXLlidXV1w55SzJzdaM6cOfbUU0/ZgQMH7MCBA/bFL37RHnjggaEPTeYsXXNzsz377LO2dOnSYa8zZ9dI8thv/dZvJY888siw1xYtWpR85zvfiXRE+cvMkhdffHHovwcHB5OqqqrkqaeeGnrt8uXLSXl5efL3f//3EY4w/7S1tSVmljQ1NSVJwpxlY+rUqck//dM/MWeOrq6upLa2NmlsbEzWrl2bPP7440mScJ9dL2+/CfX399vBgwetrq5u2Ot1dXW2f//+SEc1fhw/ftxaW1uHzV8mk7G1a9cyf/9fR0eHmZlVVFSYGXM2GgMDA7Znzx7r6emxNWvWMGeOTZs22X333Wdf+tKXhr3OnA2Xd0/RvurcuXM2MDBglZWVw16vrKy01tbWSEc1flydo5Hm78SJEzEOKa8kSWJPPPGE3XPPPbZ48WIzY848R48etTVr1tjly5etpKTEXnzxRbvrrruGPjSZs+H27Nljb775pjU3N98Q4z4bLm+T0FXXr6aYJIlcYRG/xvyN7NFHH7UjR47Ya6+9dkOMObvRwoUL7fDhw3bx4kX7t3/7N3v44YetqalpKM6c/VpLS4s9/vjjtnfvXpsyZUrq7zFnn8jbf46bPn26TZw48YZvPW1tbTf8HwRuVFVVZWbG/I3gscces5/+9Kf2i1/8YtjaVcxZusmTJ9tnP/tZW7VqlTU0NNiyZcvs+9//PnM2goMHD1pbW5utXLnSJk2aZJMmTbKmpib7u7/7O5s0adLQvDBnn8jbJDR58mRbuXKlNTY2Dnu9sbHRfvu3fzvSUY0fNTU1VlVVNWz++vv7ramp6badvyRJ7NFHH7Uf//jH9p//+Z9WU1MzLM6cjV6SJNbX18ecjeDee++1o0eP2uHDh4d+Vq1aZV/96lft8OHDNn/+fObsWvFqIrQ9e/Ykn/rUp5Lnnnsuefvtt5PNmzcnxcXFyQcffBD70PJCV1dXcujQoeTQoUOJmSU7duxIDh06lJw4cSJJkiR56qmnkvLy8uTHP/5xcvTo0eSP//iPk1mzZiWdnZ2RjzyOb37zm0l5eXnyyiuvJGfPnh36uXTp0tDvMGc32rJlS7Jv377k+PHjyZEjR5Lvfve7yYQJE5K9e/cmScKcjca11XFJwpxdK6+TUJIkyQ9+8IPkzjvvTCZPnpysWLFiqJwWSfKLX/wiMbMbfh5++OEkST4pBf3e976XVFVVJZlMJvn85z+fHD16NO5BRzTSXJlZ8vzzzw/9DnN2oz/90z8deg/OmDEjuffee4cSUJIwZ6NxfRJizn6N9YQAANHk7d+EAAC3PpIQACAakhAAIBqSEAAgGpIQACAakhAAIBqSEAAgGpIQACAakhAAIBqSEAAgGpIQACCa/wdT20/PvymwdgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth: Happinnes\n"
     ]
    }
   ],
   "source": [
    "np_img = test_dataset[88]['image']\n",
    "plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "print('image:\\n')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('truth:', classes[test_dataset[88]['emotions'].argmax(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = Net()\n",
    "my_model.load_state_dict(torch.load('./mymodel.pth'))\n",
    "my_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 69 %\n",
      "correct: 2478, total: 3572\n",
      "[tensor(0.7500, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.5000, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.9062, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.5000, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5000, device='mps:0')]\n"
     ]
    }
   ],
   "source": [
    "# test model on the test data set\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def test_model(model, test_loader, dataset):\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_accuracy = []\n",
    "    wrong = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            labels = data['emotions']\n",
    "            inputs = data['image']\n",
    "            output = model(inputs)\n",
    "            # print(\"out: \", output.shape)\n",
    "            # print(\"labels: \", labels.shape)\n",
    "            # print(labels.shape)\n",
    "            # print(f\"test loader length:  {len(test_loader)}\")\n",
    "            # show the first 10 images and their truth labels and their predictions\n",
    "            if i < len(test_loader):\n",
    "                for j in range(len(inputs)):\n",
    "                    # if classes[labels[j].argmax(0).item()] != classes[output[j].argmax(0).item()] :\n",
    "                    #     wrong +=1\n",
    "                    \n",
    "                    \n",
    "                    file_name = \"mymodel.txt\"\n",
    "                    output_file = open(file_name, \"a\")\n",
    "                    prediction = classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]\n",
    "                    \n",
    "                    output_file.write(f\"image: {i * batch_size + j} was {prediction}\\n\")\n",
    "                    output_file.write(f\"predicted label was: {classes[output[j].argmax(0).item()]} and the original label was {classes[labels[j].argmax(0).item()]}\\n\")\n",
    "                    output_file.write(\"=\"*50)\n",
    "                    output_file.write(\"\\n\")\n",
    "                    # if j == 10:\n",
    "                    #     print(\"one example:\")\n",
    "                    #     np_img = test_dataset[i * batch_size + j]['image']\n",
    "                    #     plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "                    #     plt.show()\n",
    "                    #     plt.close()\n",
    "                    #     print('-'*50)\n",
    "                        \n",
    "                    # print(labels[i].argmax().item())\n",
    "                    # if classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]:\n",
    "                    #     print(\"hoppppaaaaaaaaaa\")\n",
    "                    #     print('image:', i * batch_size + j)\n",
    "                        \n",
    "                    #     print('truth:', classes[labels[j].argmax(0).item()])\n",
    "                    #     print('prediction:', classes[output[j].argmax(0).item()])\n",
    "                    #     print('='*50)\n",
    "                    # plt.close('all')    \n",
    "            output_file.close()\n",
    "            # measure accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # torchmetrics_accuracy(predicted, labels)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_accuracy.append(torchmetrics_accuracy(predicted, labels))\n",
    "        # print total accuracy\n",
    "        print('Accuracy of the network on the test images: %d %%' % (\n",
    "            100 * correct / total))\n",
    "        print(f\"correct: {correct}, total: {total}\")\n",
    "        print(test_accuracy)\n",
    "test_model(my_model, testloader, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=10).to(device)\n",
    "\n",
    "# Calculate accuracy\n",
    "# torchmetrics_accuracy(y_preds, y_blob_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original label was Fear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 388\n",
    "\n",
    "np_img = test_dataset[idx]['image']\n",
    "plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print (f\"the original label was {classes[test_dataset[idx]['emotions'].argmax(0).item()]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- data augmentation \n",
    "2- maxing before ?\n",
    "3- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions: \n",
    "\n",
    "1- how should the final product look like? \n",
    "2- should i investigate data augmentation? \n",
    "3- how about transfer learning? any advice? \n",
    "4- REPORT, how should it look like? any examples? any guidance is appreciated\n",
    "5- in general, what do you think? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genetic programming (evolutionary algorithms) \n",
    "\n",
    "leaky Relu and param Relu\n",
    "\n",
    "regularization \n",
    "\n",
    "torch metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function softmax at 0x118dc24c0>\n"
     ]
    }
   ],
   "source": [
    "softmax = F.softmax\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
