{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.mps \n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "# plt.ion() \n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample #return a transformed image with label\n",
    "    \n",
    "    def to(self, device):\n",
    "        # move to device\n",
    "        return self.to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     class to transform to a normalized tensor (only the image pixel value is transformed)\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotions = sample['image'], sample['emotions']\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        return {'image': transform(image),\n",
    "                'emotions': emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(os.path.join(train_folder_path,\"label.csv\"), train_folder_path, transform=ToTensor())\n",
    "valid_dataset = FERPlusDataset(os.path.join(valid_folder_path, \"label.csv\"), valid_folder_path, transform=ToTensor())\n",
    "test_dataset = FERPlusDataset(os.path.join(test_folder_path, \"label.csv\"), test_folder_path, transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, scheduler ,criterion, model, trainloader, validloader, learning_rate,):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []\n",
    "    opt_name = optimizer.__name__\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if scheduler == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = scheduler(optimizer)\n",
    "        # print(\"plateu\")\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    elif scheduler == optim.lr_scheduler.ExponentialLR: \n",
    "        scheduler = scheduler(optimizer, gamma=0.9)\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    st = time.time()\n",
    "\n",
    "# Training - Validation loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        model.train()\n",
    "        # Perform training\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # labels = data['emotions']\n",
    "            # inputs = data['image']\n",
    "            labels = data['emotions'].to(device)\n",
    "            inputs = data['image'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # print(\"label before: \", labels)\n",
    "            # print(\"predicted before: \", outputs)\n",
    "            # Calculate and store training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # for performance, you can max in data prep for consistency, \n",
    "            # print(\"label: \", labels)\n",
    "            # print(\"predicted: \", predicted)\n",
    "            # print(\"pred size: \" , predicted.shape)\n",
    "            total += labels.size(0)\n",
    "            correct +=  (predicted == labels).sum().item()\n",
    "            # print(\"1 more correct..\")\n",
    "        \n",
    "              \n",
    "        scheduler.step()\n",
    "                \n",
    "        train_loss.append(running_loss / len(trainloader))\n",
    "        train_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Perform validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for data in validloader:\n",
    "                # labels = data['emotions']\n",
    "                # images = data['image']\n",
    "                labels = data['emotions'].to(device)\n",
    "                images = data['image'].to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                # print(\"total: \" , total)\n",
    "                correct +=  (predicted == labels).sum().item() #can be torch.eq(pred, labels).sum().item()\n",
    "        \n",
    "        valid_loss.append(running_loss / len(validloader))\n",
    "        valid_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Print the training and validation loss and accuracy\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {train_loss[-1]:.4f} | Training Accuracy: {train_accuracy[-1]:.2f}%')\n",
    "        print(f'Validation Loss: {valid_loss[-1]:.4f} | Validation Accuracy: {valid_accuracy[-1]:.2f}%')\n",
    "        print('-----------------------------------')\n",
    "\n",
    "    # elapsed_time = time.time() - st\n",
    "    # print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "    print('Finished Training')\n",
    "    # with open(f'{outputs_path}/statistics.csv', 'a', newline='') as csvfile:\n",
    "    #     writer = csv.writer(csvfile)\n",
    "\n",
    "    #     if csvfile.tell() == 0:\n",
    "    #         # Write the column headers\n",
    "    #         writer.writerow(['trial', 'Batch size', 'Epochs', 'Activation function', 'Loss function', 'Initial Learning rate', 'Optimizer', 'Scheduler',\n",
    "    #                         'Min training accuracy', 'Max training accuracy', 'Average training accuracy',\n",
    "    #                         'Min validation accuracy', 'Max validation accuracy', 'Average validation accuracy',\n",
    "    #                         'Total time', 'device'])\n",
    "\n",
    "  \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD\n",
    "activation = F.relu\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "scheduler = optim.lr_scheduler.ExponentialLR\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m validloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(valid_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m testloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self, drop=0.2):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) \n",
    "        # output size = 6 *44*44 values \n",
    "        # image size : n*n \n",
    "        # filter size: f*f (f is odd number)\n",
    "        # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "            self.bn1 = nn.BatchNorm2d(6)  # Batch normalization after conv1\n",
    "            \n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "        # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "        # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "            \n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #output size = 18 * 18 * 16 = 5184   \n",
    "            \n",
    "            self.bn2 = nn.BatchNorm2d(16)  # Batch normalization after conv2\n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "            \n",
    "            self.dropout = nn.Dropout(p=drop)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.pool(activation(self.bn1(self.conv1(x)))) \n",
    "            # 44*44*6 , 22*22*6 \n",
    "            \n",
    "            x = self.pool(activation(self.bn2(self.conv2(x))))\n",
    "            # 18*18*16 , 9*9*16 \n",
    "            \n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = activation(self.dropout(self.fc1(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = activation(self.dropout(self.fc2(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42) #do I need it ? wasn't in the current stats\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "print(model.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_and_validate(epochs, optimizer, scheduler , criterion, model, trainloader, validloader,learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './mymodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for classess names in FERplus dataset\n",
    "classes = {\n",
    "    0: 'Neutral',\n",
    "    1: 'Happinnes',\n",
    "    2: 'Surprise',\n",
    "    3: 'Sadness',\n",
    "    4: 'Anger',\n",
    "    5: 'Disgust',\n",
    "    6: 'Fear',\n",
    "    7: 'Contempt',\n",
    "    8: 'Unknown',\n",
    "    9: 'NF'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_img = test_dataset[32]['image']\n",
    "plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "print('image:\\n')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('truth:', classes[test_dataset[32]['emotions'].argmax(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = Net()\n",
    "my_model.load_state_dict(torch.load('./mymodel.pth'))\n",
    "my_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 69 %\n",
      "correct: 2478, wrong: 0, total: 3572\n"
     ]
    }
   ],
   "source": [
    "# test model on the test data set\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def test_model(model, test_loader, dataset):\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # test_accuracy = []\n",
    "    wrong = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            labels = data['emotions']\n",
    "            inputs = data['image']\n",
    "            output = model(inputs)\n",
    "            # print(labels.shape)\n",
    "            # print(f\"test loader length:  {len(test_loader)}\")\n",
    "            # show the first 10 images and their truth labels and their predictions\n",
    "            if i < len(test_loader):\n",
    "                for j in range(len(inputs)):\n",
    "                    # if classes[labels[j].argmax(0).item()] != classes[output[j].argmax(0).item()] :\n",
    "                    #     wrong +=1\n",
    "                    file_name = \"mymodel.txt\"\n",
    "                    output_file = open(file_name, \"a\")\n",
    "                    prediction = classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]\n",
    "                    \n",
    "                    output_file.write(f\"image: {i * batch_size + j} was {prediction}\\n\")\n",
    "                    output_file.write(f\"predicted label was: {classes[output[j].argmax(0).item()]} and the original label was {classes[labels[j].argmax(0).item()]}\\n\")\n",
    "                    output_file.write(\"=\"*50)\n",
    "                    output_file.write(\"\\n\")\n",
    "                    # if j == 10:\n",
    "                    #     print(\"one example:\")\n",
    "                    #     np_img = test_dataset[i * batch_size + j]['image']\n",
    "                    #     plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "                    #     plt.show()\n",
    "                    #     plt.close()\n",
    "                    #     print('-'*50)\n",
    "                        \n",
    "                    # print(labels[i].argmax().item())\n",
    "                    # if classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]:\n",
    "                    #     print(\"hoppppaaaaaaaaaa\")\n",
    "                    #     print('image:', i * batch_size + j)\n",
    "                        \n",
    "                    #     print('truth:', classes[labels[j].argmax(0).item()])\n",
    "                    #     print('prediction:', classes[output[j].argmax(0).item()])\n",
    "                    #     print('='*50)\n",
    "                    # plt.close('all')    \n",
    "            output_file.close()\n",
    "            # measure accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # test_accuracy.append(100 * correct / total)\n",
    "        # print total accuracy\n",
    "        print('Accuracy of the network on the test images: %d %%' % (\n",
    "            100 * correct / total))\n",
    "        print(f\"correct: {correct}, wrong: {wrong}, total: {total}\")\n",
    "test_model(my_model, testloader, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original label was Unknown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 175\n",
    "\n",
    "np_img = test_dataset[idx]['image']\n",
    "plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print (f\"the original label was {classes[test_dataset[idx]['emotions'].argmax(0).item()]}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- data augmentation \n",
    "2- maxing before ?\n",
    "3- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
