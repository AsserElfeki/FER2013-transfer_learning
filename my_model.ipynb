{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.mps \n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "# plt.ion() \n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        # image = io.imread(image)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        \n",
    "        return sample #return a transformed image with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17) #https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomApply([transforms.RandomAffine(0, translate=(0.2, 0.2))], p=0.5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#     ]),\n",
    "#     'valid': transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#     ]),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #     class to transform to a normalized tensor (only the image pixel value is transformed)\n",
    "# class ToTensor(object):\n",
    "#     \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "#     def __call__(self, sample):\n",
    "#         image, emotions = sample['image'], sample['emotions']\n",
    "#         transform = transforms.ToTensor()\n",
    "\n",
    "#         return {'image': transform(image),\n",
    "#                 'emotions': emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(csv_file= os.path.join(train_folder_path,\"label.csv\"), root_dir=train_folder_path, transform=data_transforms['train'])\n",
    "valid_dataset = FERPlusDataset(csv_file= os.path.join(valid_folder_path, \"label.csv\"), root_dir= valid_folder_path, transform=data_transforms['valid'])\n",
    "test_dataset = FERPlusDataset(csv_file= os.path.join(test_folder_path, \"label.csv\"), root_dir= test_folder_path, transform=data_transforms['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, scheduler ,criterion, model, trainloader, validloader, learning_rate,):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []\n",
    "    opt_name = optimizer.__name__\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if scheduler == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = scheduler(optimizer)\n",
    "        # print(\"plateu\")\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    elif scheduler == optim.lr_scheduler.ExponentialLR: \n",
    "        scheduler = scheduler(optimizer, gamma=0.9)\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    st = time.time()\n",
    "\n",
    "# Training - Validation loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        # Perform training\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            labels = data['emotions'].to(device)\n",
    "            inputs = data['image'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            # Calculate and store training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # for performance, you can max in data prep for consistency, \n",
    "            total += inputs.size(0)\n",
    "            correct +=  (predicted == labels).sum().item()\n",
    "        scheduler.step()\n",
    "        train_loss.append(running_loss / len(trainloader))\n",
    "        train_accuracy.append(100 * correct / total)\n",
    "        # Perform validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        with torch.inference_mode():\n",
    "            for data in validloader:\n",
    "                labels = data['emotions'].to(device)\n",
    "                inputs = data['image'].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                total += inputs.size(0)\n",
    "                correct +=  (predicted == labels).sum().item() #can be torch.eq(pred, labels).sum().item()\n",
    "        \n",
    "        valid_loss.append(running_loss / len(validloader))\n",
    "        valid_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Print the training and validation loss and accuracy\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {train_loss[-1]:.4f} | Training Accuracy: {train_accuracy[-1]:.2f}%')\n",
    "        print(f'Validation Loss: {valid_loss[-1]:.4f} | Validation Accuracy: {valid_accuracy[-1]:.2f}%')\n",
    "        print('-----------------------------------')\n",
    "\n",
    "    # elapsed_time = time.time() - st\n",
    "    print('Finished Training')\n",
    "    # with open(f'{outputs_path}/statistics.csv', 'a', newline='') as csvfile:\n",
    "    #     writer = csv.writer(csvfile)\n",
    "\n",
    "    #     if csvfile.tell() == 0:\n",
    "    #         # Write the column headers\n",
    "    #         writer.writerow(['trial', 'Batch size', 'Epochs', 'Activation function', 'Loss function', 'Initial Learning rate', 'Optimizer', 'Scheduler',\n",
    "    #                         'Min training accuracy', 'Max training accuracy', 'Average training accuracy',\n",
    "    #                         'Min validation accuracy', 'Max validation accuracy', 'Average validation accuracy',\n",
    "    #                         'Total time', 'device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = [optim.SGD, optim.Adam]\n",
    "activation = [F.relu, F.sigmoid]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "scheduler = optim.lr_scheduler.ExponentialLR\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self, drop=0.2):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) \n",
    "        # output size = 6 *44*44 values \n",
    "        # image size : n*n \n",
    "        # filter size: f*f (f is odd number)\n",
    "        # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "            self.bn1 = nn.BatchNorm2d(6)  # Batch normalization after conv1\n",
    "            \n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "        # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "        # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "            \n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #output size = 18 * 18 * 16 = 5184   \n",
    "            \n",
    "            self.bn2 = nn.BatchNorm2d(16)  # Batch normalization after conv2\n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "            \n",
    "            self.dropout = nn.Dropout(p=drop)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.pool(activation(self.bn1(self.conv1(x)))) \n",
    "            # 44*44*6 , 22*22*6 \n",
    "            \n",
    "            x = self.pool(activation(self.bn2(self.conv2(x))))\n",
    "            # 18*18*16 , 9*9*16 \n",
    "            \n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = activation(self.dropout(self.fc1(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = activation(self.dropout(self.fc2(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            # x = F.softmax(x, dim=1)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42) #do I need it ? wasn't in the current stats\n",
    "my_model = Net()\n",
    "# model.load_state_dict()\n",
    "my_model.to(device)\n",
    "\n",
    "# print(my_model.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 17.5679 | Training Accuracy: 42.03%\n",
      "Validation Loss: 15.4797 | Validation Accuracy: 54.53%\n",
      "-----------------------------------\n",
      "Epoch 2/10:\n",
      "Training Loss: 16.2709 | Training Accuracy: 50.20%\n",
      "Validation Loss: 15.3720 | Validation Accuracy: 53.30%\n",
      "-----------------------------------\n",
      "Epoch 3/10:\n",
      "Training Loss: 15.4055 | Training Accuracy: 54.97%\n",
      "Validation Loss: 13.5964 | Validation Accuracy: 63.14%\n",
      "-----------------------------------\n",
      "Epoch 4/10:\n",
      "Training Loss: 14.8784 | Training Accuracy: 57.50%\n",
      "Validation Loss: 13.1140 | Validation Accuracy: 64.62%\n",
      "-----------------------------------\n",
      "Epoch 5/10:\n",
      "Training Loss: 14.4134 | Training Accuracy: 59.82%\n",
      "Validation Loss: 12.9743 | Validation Accuracy: 64.34%\n",
      "-----------------------------------\n",
      "Epoch 6/10:\n",
      "Training Loss: 14.0902 | Training Accuracy: 61.14%\n",
      "Validation Loss: 12.8852 | Validation Accuracy: 64.56%\n",
      "-----------------------------------\n",
      "Epoch 7/10:\n",
      "Training Loss: 13.8693 | Training Accuracy: 62.43%\n",
      "Validation Loss: 12.4800 | Validation Accuracy: 66.80%\n",
      "-----------------------------------\n",
      "Epoch 8/10:\n",
      "Training Loss: 13.7013 | Training Accuracy: 63.05%\n",
      "Validation Loss: 12.0936 | Validation Accuracy: 68.45%\n",
      "-----------------------------------\n",
      "Epoch 9/10:\n",
      "Training Loss: 13.5499 | Training Accuracy: 63.50%\n",
      "Validation Loss: 12.0671 | Validation Accuracy: 67.86%\n",
      "-----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_and_validate(epochs, optimizer, scheduler , criterion, my_model, trainloader, validloader,learning_rate)\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(epochs, optimizer, scheduler, criterion, model, trainloader, validloader, learning_rate)\u001b[0m\n\u001b[1;32m     49\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m validloader:\n\u001b[1;32m     52\u001b[0m         labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39memotions\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     53\u001b[0m         inputs \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mFERPlusDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m#   to create the image name\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         img_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_frame\u001b[39m.\u001b[39miloc[idx, \u001b[39m0\u001b[39m])\n\u001b[0;32m---> 27\u001b[0m         image \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mimread(img_name)\n\u001b[1;32m     28\u001b[0m         \u001b[39m# image = io.imread(image)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(image)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/skimage/io/_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[0;32m---> 53\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39;49m\u001b[39mimread\u001b[39;49m\u001b[39m'\u001b[39;49m, fname, plugin\u001b[39m=\u001b[39;49mplugin, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mplugin_args)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/skimage/io/manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    205\u001b[0m                            (plugin, kind))\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/skimage/io/_plugins/imageio_plugin.py:15\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@wraps\u001b[39m(imageio_imread)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(imageio_imread(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/imageio/v2.py:349\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39m\u001b[39mri\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mimopen_args) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m--> 349\u001b[0m     result \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread(index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/imageio/core/v3_plugin_api.py:367\u001b[0m, in \u001b[0;36mPluginV3.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mtype\u001b[39m, value, traceback) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/imageio/plugins/pillow.py:123\u001b[0m, in \u001b[0;36mPillowPlugin.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flush_writer()\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image:\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_image\u001b[39m.\u001b[39;49mclose()\n\u001b[1;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/PIL/Image.py:587\u001b[0m, in \u001b[0;36mImage.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_fp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    586\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp:\n\u001b[0;32m--> 587\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mclose()\n\u001b[1;32m    588\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp \u001b[39m=\u001b[39m DeferredError(\u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOperation on closed image\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_validate(epochs, optimizer, scheduler , criterion, my_model, trainloader, validloader,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(my_model.state_dict(), './models/myCNN_noAug_30_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for classess names in FERplus dataset\n",
    "classes = {\n",
    "    0: 'Neutral',\n",
    "    1: 'Happinnes',\n",
    "    2: 'Surprise',\n",
    "    3: 'Sadness',\n",
    "    4: 'Anger',\n",
    "    5: 'Disgust',\n",
    "    6: 'Fear',\n",
    "    7: 'Contempt',\n",
    "    8: 'Unknown',\n",
    "    9: 'NF'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neutral = 0\n",
    "Happinnes = 0\n",
    "Surprise = 0\n",
    "Sadness = 0\n",
    "Anger = 0\n",
    "Disgust = 0\n",
    "Fear = 0\n",
    "Contempt = 0\n",
    "Unknown = 0\n",
    "NF = 0\n",
    "total = 0\n",
    "\n",
    "# count classes in train_dataset\n",
    "\n",
    "\n",
    "for batch in testloader: \n",
    "    # for data in batch['emotions']:\n",
    "    labels = batch['emotions']\n",
    "    # print(labels.shape)\n",
    "    _, labels = torch.max(labels,1)\n",
    "    for label in labels: \n",
    "        # print(label)\n",
    "        new_label = classes[label.item()]\n",
    "        total +=1\n",
    "        if new_label == classes[0]: \n",
    "            Neutral +=1 \n",
    "        elif new_label == classes[1]:\n",
    "            Happinnes +=1\n",
    "        elif new_label == classes[2]:\n",
    "            Surprise +=1\n",
    "        elif new_label == classes[3]:\n",
    "            Sadness +=1\n",
    "        elif new_label == classes[4]:\n",
    "            Anger +=1\n",
    "        elif new_label == classes[5]:\n",
    "            Disgust +=1\n",
    "        elif new_label == classes[6]:\n",
    "            Fear +=1\n",
    "        elif new_label == classes[7]:\n",
    "            Contempt +=1\n",
    "        elif new_label == classes[8]:\n",
    "            Unknown +=1\n",
    "        elif new_label == classes[9]:\n",
    "            NF +=1\n",
    "\n",
    "print(f\" Happiness: {Happinnes}, Nuetral : {Neutral}, Anger: {Anger}, Sadness: {Sadness}, Fear: {Fear}, Disgust: {Disgust}, Contempt: {Contempt}, NF: {NF}, Unknown: {Unknown}\")  \n",
    "print(f\"total: {total}\")                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 100\n",
    "# for idx in range(100):\n",
    "#     np_img = train_dataset[idx]['image']\n",
    "#     plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "#     print(\"image:\",idx,  'truth:', classes[train_dataset[idx]['emotions'].argmax(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = Net()\n",
    "my_model.load_state_dict(torch.load('./models/myCNN_aug_200_epochs.pth'))\n",
    "my_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'model.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 71 %\n",
      "correct: 2545, total: 3572\n",
      "[tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6000, device='mps:0')]\n",
      "tensor(0.7121, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# test model on the test data set\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def test_model(model, test_loader, dataset):\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_accuracy = []\n",
    "    wrong = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            labels = data['emotions']\n",
    "            inputs = data['image']\n",
    "            output = model(inputs)\n",
    "            # print(\"out: \", output.shape)\n",
    "            # print(\"labels: \", labels.shape)\n",
    "            # print(labels.shape)\n",
    "            # print(f\"test loader length:  {len(test_loader)}\")\n",
    "            # show the first 10 images and their truth labels and their predictions\n",
    "            if i < len(test_loader):\n",
    "                for j in range(len(inputs)):\n",
    "                    # if classes[labels[j].argmax(0).item()] != classes[output[j].argmax(0).item()] :\n",
    "                    #     wrong +=1\n",
    "                    \n",
    "                    output_file = open(out_file, \"a\")\n",
    "                    prediction = classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]\n",
    "                    \n",
    "                    output_file.write(f\"image: {i * batch_size + j} was {prediction}\\n\")\n",
    "                    output_file.write(f\"predicted label was: {classes[output[j].argmax(0).item()]} and the original label was {classes[labels[j].argmax(0).item()]}\\n\")\n",
    "                    output_file.write(\"=\"*50)\n",
    "                    output_file.write(\"\\n\")\n",
    "                    # if j == 10:\n",
    "                    #     print(\"one example:\")\n",
    "                    #     np_img = test_dataset[i * batch_size + j]['image']\n",
    "                    #     plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "                    #     plt.show()\n",
    "                    #     plt.close()\n",
    "                    #     print('-'*50)\n",
    "                        \n",
    "                    # print(labels[i].argmax().item())\n",
    "                    # if classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]:\n",
    "                    #     print(\"hoppppaaaaaaaaaa\")\n",
    "                    #     print('image:', i * batch_size + j)\n",
    "                        \n",
    "                    #     print('truth:', classes[labels[j].argmax(0).item()])\n",
    "                    #     print('prediction:', classes[output[j].argmax(0).item()])\n",
    "                    #     print('='*50)\n",
    "                    # plt.close('all')    \n",
    "            output_file.close()\n",
    "            # measure accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # torchmetrics_accuracy(predicted, labels)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_accuracy.append(torchmetrics_accuracy(predicted, labels))\n",
    "        # print total accuracy\n",
    "        print('Accuracy of the network on the test images: %d %%' % (\n",
    "            100 * correct / total))\n",
    "        print(f\"correct: {correct}, total: {total}\")\n",
    "        print(test_accuracy)\n",
    "        print(sum(test_accuracy) / len(test_accuracy))\n",
    "test_model(my_model, testloader, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 388\n",
    "\n",
    "# np_img = test_dataset[idx]['image']\n",
    "# plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "# print (f\"the original label was {classes[test_dataset[idx]['emotions'].argmax(0).item()]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- data augmentation \n",
    "2- maxing before ?\n",
    "3- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions: \n",
    "\n",
    "1- how should the final product look like? \n",
    "2- should i investigate data augmentation? \n",
    "3- how about transfer learning? any advice? \n",
    "4- REPORT, how should it look like? any examples? any guidance is appreciated\n",
    "5- in general, what do you think? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genetic programming (evolutionary algorithms) \n",
    "\n",
    "leaky Relu and param Relu\n",
    "\n",
    "regularization \n",
    "\n",
    "torch metrics "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
