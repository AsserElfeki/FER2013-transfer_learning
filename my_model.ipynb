{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.mps \n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "# plt.ion() \n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample #return a transformed image with label\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     class to transform to a normalized tensor (only the image pixel value is transformed)\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotions = sample['image'], sample['emotions']\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        return {'image': transform(image),\n",
    "                'emotions': emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(os.path.join(train_folder_path,\"label.csv\"), train_folder_path, transform=ToTensor())\n",
    "valid_dataset = FERPlusDataset(os.path.join(valid_folder_path, \"label.csv\"), valid_folder_path, transform=ToTensor())\n",
    "test_dataset = FERPlusDataset(os.path.join(test_folder_path, \"label.csv\"), test_folder_path, transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, scheduler ,criterion, model, trainloader, validloader, learning_rate,):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []\n",
    "    opt_name = optimizer.__name__\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if scheduler == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = scheduler(optimizer)\n",
    "        # print(\"plateu\")\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    elif scheduler == optim.lr_scheduler.ExponentialLR: \n",
    "        scheduler = scheduler(optimizer, gamma=0.9)\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    st = time.time()\n",
    "\n",
    "# Training - Validation loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        model.train()\n",
    "        # Perform training\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # labels = data['emotions']\n",
    "            # inputs = data['image']\n",
    "            labels = data['emotions'].to(device)\n",
    "            inputs = data['image'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # print(\"label before: \", labels)\n",
    "            # print(\"predicted before: \", outputs)\n",
    "            # Calculate and store training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # for performance, you can max in data prep for consistency, \n",
    "            # print(\"label: \", labels)\n",
    "            # print(\"predicted: \", predicted)\n",
    "            # print(\"pred size: \" , predicted.shape)\n",
    "            total += labels.size(0)\n",
    "            correct +=  (predicted == labels).sum().item()\n",
    "            # print(\"1 more correct..\")\n",
    "        \n",
    "              \n",
    "        scheduler.step()\n",
    "                \n",
    "        train_loss.append(running_loss / len(trainloader))\n",
    "        train_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Perform validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for data in validloader:\n",
    "                # labels = data['emotions']\n",
    "                # images = data['image']\n",
    "                labels = data['emotions'].to(device)\n",
    "                images = data['image'].to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                # print(\"total: \" , total)\n",
    "                correct +=  (predicted == labels).sum().item() #can be torch.eq(pred, labels).sum().item()\n",
    "        \n",
    "        valid_loss.append(running_loss / len(validloader))\n",
    "        valid_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Print the training and validation loss and accuracy\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {train_loss[-1]:.4f} | Training Accuracy: {train_accuracy[-1]:.2f}%')\n",
    "        print(f'Validation Loss: {valid_loss[-1]:.4f} | Validation Accuracy: {valid_accuracy[-1]:.2f}%')\n",
    "        print('-----------------------------------')\n",
    "\n",
    "    # elapsed_time = time.time() - st\n",
    "    # print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "    print('Finished Training')\n",
    "    # with open(f'{outputs_path}/statistics.csv', 'a', newline='') as csvfile:\n",
    "    #     writer = csv.writer(csvfile)\n",
    "\n",
    "    #     if csvfile.tell() == 0:\n",
    "    #         # Write the column headers\n",
    "    #         writer.writerow(['trial', 'Batch size', 'Epochs', 'Activation function', 'Loss function', 'Initial Learning rate', 'Optimizer', 'Scheduler',\n",
    "    #                         'Min training accuracy', 'Max training accuracy', 'Average training accuracy',\n",
    "    #                         'Min validation accuracy', 'Max validation accuracy', 'Average validation accuracy',\n",
    "    #                         'Total time', 'device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD\n",
    "activation = F.relu\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "scheduler = optim.lr_scheduler.ExponentialLR\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self, drop=0.2):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) \n",
    "        # output size = 6 *44*44 values \n",
    "        # image size : n*n \n",
    "        # filter size: f*f (f is odd number)\n",
    "        # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "            self.bn1 = nn.BatchNorm2d(6)  # Batch normalization after conv1\n",
    "            \n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "        # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "        # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "            \n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #output size = 18 * 18 * 16 = 5184   \n",
    "            \n",
    "            self.bn2 = nn.BatchNorm2d(16)  # Batch normalization after conv2\n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "            \n",
    "            self.dropout = nn.Dropout(p=drop)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.pool(activation(self.bn1(self.conv1(x)))) \n",
    "            # 44*44*6 , 22*22*6 \n",
    "            \n",
    "            x = self.pool(activation(self.bn2(self.conv2(x))))\n",
    "            # 18*18*16 , 9*9*16 \n",
    "            \n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = activation(self.dropout(self.fc1(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = activation(self.dropout(self.fc2(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42) #do I need it ? wasn't in the current stats\n",
    "model = Net()\n",
    "# model.load_state_dict()\n",
    "model.to(device)\n",
    "\n",
    "print(model.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_and_validate(epochs, optimizer, scheduler , criterion, model, trainloader, validloader,learning_rate)\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(epochs, optimizer, scheduler, criterion, model, trainloader, validloader, learning_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     27\u001b[0m \u001b[39m# Perform training\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[39m# labels = data['emotions']\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39m# inputs = data['image']\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39memotions\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m     inputs \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mFERPlusDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m#   to create the image name\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         img_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_frame\u001b[39m.\u001b[39miloc[idx, \u001b[39m0\u001b[39m])\n\u001b[0;32m---> 27\u001b[0m         image \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mimread(img_name)\n\u001b[1;32m     28\u001b[0m         emotions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_frame\u001b[39m.\u001b[39miloc[idx, \u001b[39m2\u001b[39m:]\n\u001b[1;32m     29\u001b[0m         emotions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(emotions)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/skimage/io/_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[0;32m---> 53\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39;49m\u001b[39mimread\u001b[39;49m\u001b[39m'\u001b[39;49m, fname, plugin\u001b[39m=\u001b[39;49mplugin, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mplugin_args)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/skimage/io/manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    205\u001b[0m                            (plugin, kind))\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/skimage/io/_plugins/imageio_plugin.py:15\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@wraps\u001b[39m(imageio_imread)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(imageio_imread(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/imageio/v2.py:348\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m imopen_args \u001b[39m=\u001b[39m decypher_format_arg(\u001b[39mformat\u001b[39m)\n\u001b[1;32m    346\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39;49m\u001b[39mri\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mimopen_args) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m    349\u001b[0m     result \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread(index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/imageio/core/imopen.py:196\u001b[0m, in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     plugin_instance \u001b[39m=\u001b[39m candidate_plugin(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    197\u001b[0m \u001b[39mexcept\u001b[39;00m InitializationError:\n\u001b[1;32m    198\u001b[0m     \u001b[39m# file extension doesn't match file type\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/imageio/plugins/pillow.py:80\u001b[0m, in \u001b[0;36mPillowPlugin.__init__\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m request\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mio_mode \u001b[39m==\u001b[39m IOMode\u001b[39m.\u001b[39mread:\n\u001b[1;32m     79\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[39mwith\u001b[39;00m Image\u001b[39m.\u001b[39;49mopen(request\u001b[39m.\u001b[39;49mget_file()):\n\u001b[1;32m     81\u001b[0m             \u001b[39m# Check if it is generally possible to read the image.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m             \u001b[39m# This will not read any data and merely try to find a\u001b[39;00m\n\u001b[1;32m     83\u001b[0m             \u001b[39m# compatible pillow plugin (ref: the pillow docs).\u001b[39;00m\n\u001b[1;32m     84\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[39mexcept\u001b[39;00m UnidentifiedImageError:\n",
      "File \u001b[0;32m~/dev/ml/env/lib/python3.8/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     fp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fp\u001b[39m.\u001b[39mread())\n\u001b[1;32m   3234\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 3236\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(\u001b[39m16\u001b[39;49m)\n\u001b[1;32m   3238\u001b[0m preinit()\n\u001b[1;32m   3240\u001b[0m accept_warnings \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_validate(epochs, optimizer, scheduler , criterion, model, trainloader, validloader,learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './mymodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for classess names in FERplus dataset\n",
    "classes = {\n",
    "    0: 'Neutral',\n",
    "    1: 'Happinnes',\n",
    "    2: 'Surprise',\n",
    "    3: 'Sadness',\n",
    "    4: 'Anger',\n",
    "    5: 'Disgust',\n",
    "    6: 'Fear',\n",
    "    7: 'Contempt',\n",
    "    8: 'Unknown',\n",
    "    9: 'NF'\n",
    "}\n",
    "\n",
    "# how to export this and imprt it in another file?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth: Happinnes\n"
     ]
    }
   ],
   "source": [
    "np_img = test_dataset[88]['image']\n",
    "plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "print('image:\\n')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('truth:', classes[test_dataset[88]['emotions'].argmax(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = Net()\n",
    "my_model.load_state_dict(torch.load('./mymodel.pth'))\n",
    "my_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 69 %\n",
      "correct: 2478, total: 3572\n",
      "[tensor(0.7500, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.5000, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.9062, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.5000, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.5312, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.5000, device='mps:0')]\n"
     ]
    }
   ],
   "source": [
    "# test model on the test data set\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def test_model(model, test_loader, dataset):\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_accuracy = []\n",
    "    wrong = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            labels = data['emotions']\n",
    "            inputs = data['image']\n",
    "            output = model(inputs)\n",
    "            # print(\"out: \", output.shape)\n",
    "            # print(\"labels: \", labels.shape)\n",
    "            # print(labels.shape)\n",
    "            # print(f\"test loader length:  {len(test_loader)}\")\n",
    "            # show the first 10 images and their truth labels and their predictions\n",
    "            if i < len(test_loader):\n",
    "                for j in range(len(inputs)):\n",
    "                    # if classes[labels[j].argmax(0).item()] != classes[output[j].argmax(0).item()] :\n",
    "                    #     wrong +=1\n",
    "                    \n",
    "                    \n",
    "                    file_name = \"mymodel.txt\"\n",
    "                    output_file = open(file_name, \"a\")\n",
    "                    prediction = classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]\n",
    "                    \n",
    "                    output_file.write(f\"image: {i * batch_size + j} was {prediction}\\n\")\n",
    "                    output_file.write(f\"predicted label was: {classes[output[j].argmax(0).item()]} and the original label was {classes[labels[j].argmax(0).item()]}\\n\")\n",
    "                    output_file.write(\"=\"*50)\n",
    "                    output_file.write(\"\\n\")\n",
    "                    # if j == 10:\n",
    "                    #     print(\"one example:\")\n",
    "                    #     np_img = test_dataset[i * batch_size + j]['image']\n",
    "                    #     plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "                    #     plt.show()\n",
    "                    #     plt.close()\n",
    "                    #     print('-'*50)\n",
    "                        \n",
    "                    # print(labels[i].argmax().item())\n",
    "                    # if classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]:\n",
    "                    #     print(\"hoppppaaaaaaaaaa\")\n",
    "                    #     print('image:', i * batch_size + j)\n",
    "                        \n",
    "                    #     print('truth:', classes[labels[j].argmax(0).item()])\n",
    "                    #     print('prediction:', classes[output[j].argmax(0).item()])\n",
    "                    #     print('='*50)\n",
    "                    # plt.close('all')    \n",
    "            output_file.close()\n",
    "            # measure accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # torchmetrics_accuracy(predicted, labels)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_accuracy.append(torchmetrics_accuracy(predicted, labels))\n",
    "        # print total accuracy\n",
    "        print('Accuracy of the network on the test images: %d %%' % (\n",
    "            100 * correct / total))\n",
    "        print(f\"correct: {correct}, total: {total}\")\n",
    "        print(test_accuracy)\n",
    "test_model(my_model, testloader, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=10).to(device)\n",
    "\n",
    "# Calculate accuracy\n",
    "# torchmetrics_accuracy(y_preds, y_blob_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original label was Happinnes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 88\n",
    "\n",
    "np_img = test_dataset[idx]['image']\n",
    "plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print (f\"the original label was {classes[test_dataset[idx]['emotions'].argmax(0).item()]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- data augmentation \n",
    "2- maxing before ?\n",
    "3- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions: \n",
    "\n",
    "1- how should the final product look like? \n",
    "2- should i investigate data augmentation? \n",
    "3- how about transfer learning? any advice? \n",
    "4- REPORT, how should it look like? any examples? any guidance is appreciated\n",
    "5- in general, what do you think? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genetic programming (evolutionary algorithms) \n",
    "\n",
    "leaky Relu and param Relu\n",
    "\n",
    "regularization \n",
    "\n",
    "torch metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function softmax at 0x118dc24c0>\n"
     ]
    }
   ],
   "source": [
    "softmax = F.softmax\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
