{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.mps \n",
    "import itertools\n",
    "import csv\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# plt.ion() \n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS found\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        # image = io.imread(image)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        \n",
    "        return sample #return a transformed image with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17) #https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomApply([transforms.RandomAffine(0, translate=(0.2, 0.2))], p=0.5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#     ]),\n",
    "#     'valid': transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#     ]),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(csv_file= os.path.join(train_folder_path,\"label.csv\"), root_dir=train_folder_path, transform=data_transforms['train'])\n",
    "valid_dataset = FERPlusDataset(csv_file= os.path.join(valid_folder_path, \"label.csv\"), root_dir= valid_folder_path, transform=data_transforms['valid'])\n",
    "test_dataset = FERPlusDataset(csv_file= os.path.join(test_folder_path, \"label.csv\"), root_dir= test_folder_path, transform=data_transforms['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(epochs, optimizer, scheduler ,criterion, model, trainloader, validloader, learning_rate,):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []\n",
    "    opt_name = optimizer.__name__\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if scheduler == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = scheduler(optimizer)\n",
    "        # print(\"plateu\")\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    elif scheduler == optim.lr_scheduler.ExponentialLR: \n",
    "        scheduler = scheduler(optimizer, gamma=0.9)\n",
    "        # print(type(scheduler))\n",
    "        \n",
    "    st = time.time()\n",
    "\n",
    "# Training - Validation loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        # Perform training\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            labels = data['emotions'].to(device)\n",
    "            inputs = data['image'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            # Calculate and store training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # for performance, you can max in data prep for consistency, \n",
    "            total += inputs.size(0)\n",
    "            correct +=  (predicted == labels).sum().item()\n",
    "        scheduler.step()\n",
    "        train_loss.append(running_loss / len(trainloader))\n",
    "        train_accuracy.append(100 * correct / total)\n",
    "        # Perform validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        with torch.inference_mode():\n",
    "            for data in validloader:\n",
    "                labels = data['emotions'].to(device)\n",
    "                inputs = data['image'].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                total += inputs.size(0)\n",
    "                correct +=  (predicted == labels).sum().item() #can be torch.eq(pred, labels).sum().item()\n",
    "        \n",
    "        valid_loss.append(running_loss / len(validloader))\n",
    "        valid_accuracy.append(100 * correct / total)\n",
    "        \n",
    "        # Print the training and validation loss and accuracy\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {train_loss[-1]:.4f} | Training Accuracy: {train_accuracy[-1]:.2f}%')\n",
    "        print(f'Validation Loss: {valid_loss[-1]:.4f} | Validation Accuracy: {valid_accuracy[-1]:.2f}%')\n",
    "        print('-----------------------------------')\n",
    "\n",
    "    # elapsed_time = time.time() - st\n",
    "    print('Finished Training')\n",
    "    # with open(f'{outputs_path}/statistics.csv', 'a', newline='') as csvfile:\n",
    "    #     writer = csv.writer(csvfile)\n",
    "\n",
    "    #     if csvfile.tell() == 0:\n",
    "    #         # Write the column headers\n",
    "    #         writer.writerow(['trial', 'Batch size', 'Epochs', 'Activation function', 'Loss function', 'Initial Learning rate', 'Optimizer', 'Scheduler',\n",
    "    #                         'Min training accuracy', 'Max training accuracy', 'Average training accuracy',\n",
    "    #                         'Min validation accuracy', 'Max validation accuracy', 'Average validation accuracy',\n",
    "    #                         'Total time', 'device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = [optim.SGD, optim.Adam]\n",
    "activation = [F.relu, F.sigmoid]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "scheduler = optim.lr_scheduler.ExponentialLR\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self, drop=0.2):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) \n",
    "        # output size = 6 *44*44 values \n",
    "        # image size : n*n \n",
    "        # filter size: f*f (f is odd number)\n",
    "        # shrinked_image size : (n - f + 1)^2 \n",
    "\n",
    "            self.bn1 = nn.BatchNorm2d(6)  # Batch normalization after conv1\n",
    "            \n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "        # default stride is 2 because it was not specified so defaults to kernel size which is 2\n",
    "        # output size = ((n-f+1)/2)^2 = 22*22 *6  \n",
    "            \n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #output size = 18 * 18 * 16 = 5184   \n",
    "            \n",
    "            self.bn2 = nn.BatchNorm2d(16)  # Batch normalization after conv2\n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 9 * 9, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "            \n",
    "            self.dropout = nn.Dropout(p=drop)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.pool(F.relu(self.bn1(self.conv1(x)))) \n",
    "            # 44*44*6 , 22*22*6 \n",
    "            \n",
    "            x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "            # 18*18*16 , 9*9*16 \n",
    "            \n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = F.relu(self.dropout(self.fc1(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = F.relu(self.dropout(self.fc2(x)))\n",
    "            # x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            # x = F.softmax(x, dim=1)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42) #do I need it ? wasn't in the current stats\n",
    "my_model = Net()\n",
    "# model.load_state_dict()\n",
    "my_model.to(device)\n",
    "\n",
    "# print(my_model.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_and_validate(epochs, optimizer, scheduler , criterion, my_model, trainloader, validloader,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(my_model.state_dict(), './models/myCNN_noAug_30_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dictionary for classess names in FERplus dataset\n",
    "classes_map = {\n",
    "    0: 'Neutral',\n",
    "    1: 'Happinnes',\n",
    "    2: 'Surprise',\n",
    "    3: 'Sadness',\n",
    "    4: 'Anger',\n",
    "    5: 'Disgust',\n",
    "    6: 'Fear',\n",
    "    7: 'Contempt',\n",
    "    8: 'Unknown',\n",
    "    9: 'NF'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neutral = 0\n",
    "# Happinnes = 0\n",
    "# Surprise = 0\n",
    "# Sadness = 0\n",
    "# Anger = 0\n",
    "# Disgust = 0\n",
    "# Fear = 0\n",
    "# Contempt = 0\n",
    "# Unknown = 0\n",
    "# NF = 0\n",
    "# total = 0\n",
    "\n",
    "# # count classes in train_dataset\n",
    "\n",
    "\n",
    "# for batch in testloader: \n",
    "#     # for data in batch['emotions']:\n",
    "#     labels = batch['emotions']\n",
    "#     # print(labels.shape)\n",
    "#     _, labels = torch.max(labels,1)\n",
    "#     for label in labels: \n",
    "#         # print(label)\n",
    "#         new_label = classes[label.item()]\n",
    "#         total +=1\n",
    "#         if new_label == classes[0]: \n",
    "#             Neutral +=1 \n",
    "#         elif new_label == classes[1]:\n",
    "#             Happinnes +=1\n",
    "#         elif new_label == classes[2]:\n",
    "#             Surprise +=1\n",
    "#         elif new_label == classes[3]:\n",
    "#             Sadness +=1\n",
    "#         elif new_label == classes[4]:\n",
    "#             Anger +=1\n",
    "#         elif new_label == classes[5]:\n",
    "#             Disgust +=1\n",
    "#         elif new_label == classes[6]:\n",
    "#             Fear +=1\n",
    "#         elif new_label == classes[7]:\n",
    "#             Contempt +=1\n",
    "#         elif new_label == classes[8]:\n",
    "#             Unknown +=1\n",
    "#         elif new_label == classes[9]:\n",
    "#             NF +=1\n",
    "\n",
    "# print(f\" Happiness: {Happinnes}, Nuetral : {Neutral}, Anger: {Anger}, Sadness: {Sadness}, Fear: {Fear}, Disgust: {Disgust}, Contempt: {Contempt}, NF: {NF}, Unknown: {Unknown}\")  \n",
    "# print(f\"total: {total}\")                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 100\n",
    "# for idx in range(100):\n",
    "#     np_img = train_dataset[idx]['image']\n",
    "#     plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "#     print(\"image:\",idx,  'truth:', classes[train_dataset[idx]['emotions'].argmax(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1296, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = Net()\n",
    "my_model.load_state_dict(torch.load('./models/myCNN_aug_200_epochs.pth'))\n",
    "# my_model.load_state_dict(torch.load('./models/CNN/trial_5.pth'))\n",
    "\n",
    "my_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'model.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'Neutral',\n",
    "    'Happinnes',\n",
    "    'Surprise',\n",
    "    'Sadness',\n",
    "    'Anger',\n",
    "    'Disgust',\n",
    "    'Fear',\n",
    "    'Contempt',\n",
    "    'Unknown', \n",
    "    # 'NF'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 107\u001b[0m\n\u001b[1;32m     84\u001b[0m         plt\u001b[39m.\u001b[39msavefig(save_path)\n\u001b[1;32m     88\u001b[0m         \u001b[39m# Log evaluation metrics\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[39m# print(\"Evaluation Metrics:\")\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[39m# print(f\"average accuracy : {100* (sum(test_accuracy) / len(test_accuracy)):.2f}%\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[39m# print(test_accuracy)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m         \u001b[39m# print(sum(test_accuracy) / len(test_accuracy))\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m test_model(model, test_loader\u001b[39m=\u001b[39mtestloader)\n\u001b[1;32m    108\u001b[0m plt\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# test model on the test data set\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def test_model(model, test_loader):\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_accuracy = []\n",
    "    predicted_labels=[]\n",
    "    true_labels = []\n",
    "    \n",
    "    wrong = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            labels = data['emotions']\n",
    "            inputs = data['image']\n",
    "            output = model(inputs)\n",
    "            # print(\"out: \", output.shape)\n",
    "            # print(\"labels: \", labels.shape)\n",
    "            # print(labels.shape)\n",
    "            # print(f\"test loader length:  {len(test_loader)}\")\n",
    "            # show the first 10 images and their truth labels and their predictions\n",
    "            # if i < len(test_loader):\n",
    "            #     for j in range(len(inputs)):\n",
    "                    # if classes[labels[j].argmax(0).item()] != classes[output[j].argmax(0).item()] :\n",
    "                    #     wrong +=1\n",
    "                    \n",
    "                    # output_file = open(out_file, \"a\")\n",
    "                    # prediction = classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]\n",
    "                    \n",
    "                    # output_file.write(f\"image: {i * batch_size + j} was {prediction}\\n\")\n",
    "                    # output_file.write(f\"predicted label was: {classes[output[j].argmax(0).item()]} and the original label was {classes[labels[j].argmax(0).item()]}\\n\")\n",
    "                    # output_file.write(\"=\"*50)\n",
    "                    # output_file.write(\"\\n\")\n",
    "                    # if j == 10:\n",
    "                    #     print(\"one example:\")\n",
    "                    #     np_img = test_dataset[i * batch_size + j]['image']\n",
    "                    #     plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "                    #     plt.show()\n",
    "                    #     plt.close()\n",
    "                    #     print('-'*50)\n",
    "                        \n",
    "                    # print(labels[i].argmax().item())\n",
    "                    # if classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]:\n",
    "                    #     print(\"hoppppaaaaaaaaaa\")\n",
    "                    #     print('image:', i * batch_size + j)\n",
    "                        \n",
    "                    #     print('truth:', classes[labels[j].argmax(0).item()])\n",
    "                    #     print('prediction:', classes[output[j].argmax(0).item()])\n",
    "                    #     print('='*50)\n",
    "                    # plt.close('all')    \n",
    "            # output_file.close()\n",
    "            # measure accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # torchmetrics_accuracy(predicted, labels)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_accuracy.append(torchmetrics_accuracy(predicted, labels))\n",
    "            \n",
    "            # Store predicted and true labels for calculating metrics\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        # print(f\"labels pred: {len(predicted_labels) }\")\n",
    "        # print(f\"labeld true: {len(true_labels) }\")\n",
    "            \n",
    "            # Calculate evaluation metrics\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "        precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "        recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "        # confusion_mat = confusion_matrix(true_labels, predicted_labels)\n",
    "        # ConfusionMatrixDisplay.from_predictions(true_labels, predicted_labels, normalize='true', display_labels=classes, cmap='Blues', xticks_rotation=45.0)\n",
    "        \n",
    "        cm_display = ConfusionMatrixDisplay.from_predictions(y_true=true_labels, y_pred=predicted_labels, normalize='true', display_labels=classes, cmap='Blues', values_format='0.2f', xticks_rotation=45)\n",
    "        fig, ax = plt.subplots(figsize=(10,8))\n",
    "        cm_display.plot(ax=ax)\n",
    "\n",
    "        # Save the plot to a file\n",
    "        save_path = 'confusion_matrix3.png'  # Specify the desired save path\n",
    "        plt.savefig(save_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        # print(\"Evaluation Metrics:\")\n",
    "        # print(f\"average accuracy : {100* (sum(test_accuracy) / len(test_accuracy)):.2f}%\")\n",
    "        # print(f\"F1 Score: {100* f1 :.2f}%\")\n",
    "        # print(f\"Precision: {100* precision :.2f}%\")\n",
    "        # print(f\"Recall: {100* recall :.2f}%\")\n",
    "        # print(\"Confusion Matrix:\")\n",
    "        # # print(confusion_mat)\n",
    "        # # sns.heatmap(confusion_mat, annot=True, cmap=\"Blues\")\n",
    "        # # disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=classes, xticks_rotation='vertical',normalize='true' )\n",
    "        # # disp.plot()\n",
    "        # # plt.show()\n",
    "        \n",
    "        # # print total accuracy\n",
    "        # print('Accuracy of the network on the test images: %d %%' % (\n",
    "        #     100 * correct / total))\n",
    "        # print(f\"correct: {correct}, total: {total}\")\n",
    "        # print(test_accuracy)\n",
    "        # print(sum(test_accuracy) / len(test_accuracy))\n",
    "test_model(model, test_loader=testloader)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 388\n",
    "\n",
    "# np_img = test_dataset[idx]['image']\n",
    "# plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "# print (f\"the original label was {classes[test_dataset[idx]['emotions'].argmax(0).item()]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- data augmentation \n",
    "2- maxing before ?\n",
    "3- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions: \n",
    "\n",
    "1- how should the final product look like? \n",
    "2- should i investigate data augmentation? \n",
    "3- how about transfer learning? any advice? \n",
    "4- REPORT, how should it look like? any examples? any guidance is appreciated\n",
    "5- in general, what do you think? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genetic programming (evolutionary algorithms) \n",
    "\n",
    "leaky Relu and param Relu\n",
    "\n",
    "regularization \n",
    "\n",
    "torch metrics "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
