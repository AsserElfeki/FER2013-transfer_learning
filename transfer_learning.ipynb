{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.mps \n",
    "import itertools\n",
    "import csv\n",
    "from torchmetrics import Accuracy\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS found\")\n",
    "    device = torch.device(\"mps\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = './data/FER2013Train'\n",
    "test_folder_path = './data/FER2013Test'\n",
    "valid_folder_path = './data/FER2013Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"FERPlus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get the unique classes from the emotions column\n",
    "        self.classes = np.unique(self.img_frame.iloc[:, 2:]).tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_frame)\n",
    "\n",
    "#     to access elements using the []\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#   to create the image name\n",
    "        img_name = os.path.join(self.root_dir, self.img_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        emotions = self.img_frame.iloc[idx, 2:]\n",
    "        emotions = np.asarray(emotions)\n",
    "        emotions = emotions.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'emotions': emotions} # a dictionary of an image with its label\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample #return a transformed image with label\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotions = sample['image'], sample['emotions']\n",
    "\n",
    "        # Convert grayscale image to RGB\n",
    "        image_rgb = np.repeat(image[..., np.newaxis], 3, axis=-1)\n",
    "\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        return {'image': transform(image_rgb),\n",
    "                'emotions': emotions}\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = FERPlusDataset(os.path.join(train_folder_path,\"label.csv\"), train_folder_path, transform=ToTensor())\n",
    "valid_dataset = FERPlusDataset(os.path.join(valid_folder_path, \"label.csv\"), valid_folder_path, transform=ToTensor())\n",
    "test_dataset = FERPlusDataset(os.path.join(test_folder_path, \"label.csv\"), test_folder_path, transform=ToTensor())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the emotion labels\n",
    "emotion_labels = ['neutral', 'happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear', 'contempt', 'unknown', 'NF']\n",
    "\n",
    "# Randomly select and display images from the train set\n",
    "train_indices = np.random.choice(len(train_dataset), size=5, replace=False)\n",
    "train_images = [train_dataset[i]['image'] for i in train_indices]\n",
    "train_labels = [train_dataset[i]['emotions'] for i in train_indices]\n",
    "\n",
    "# Randomly select and display images from the validation set\n",
    "valid_indices = np.random.choice(len(valid_dataset), size=5, replace=False)\n",
    "valid_images = [valid_dataset[i]['image'] for i in valid_indices]\n",
    "valid_labels = [valid_dataset[i]['emotions'] for i in valid_indices]\n",
    "\n",
    "# Randomly select and display images from the test set\n",
    "test_indices = np.random.choice(len(test_dataset), size=5, replace=False)\n",
    "test_images = [test_dataset[i]['image'] for i in test_indices]\n",
    "test_labels = [test_dataset[i]['emotions'] for i in test_indices]\n",
    "\n",
    "# Display the images and labels\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 8))\n",
    "\n",
    "for i, (image, label) in enumerate(zip(train_images, train_labels)):\n",
    "    axes[0, i].imshow(image.permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title('Train Image')\n",
    "    axes[0, i].text(0, 5, emotion_labels[np.argmax(label)], color='red')\n",
    "\n",
    "for i, (image, label) in enumerate(zip(valid_images, valid_labels)):\n",
    "    axes[1, i].imshow(image.permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title('Valid Image')\n",
    "    axes[1, i].text(0, 5, emotion_labels[np.argmax(label)], color='red')\n",
    "\n",
    "for i, (image, label) in enumerate(zip(test_images, test_labels)):\n",
    "    axes[2, i].imshow(image.permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "    axes[2, i].set_title('Test Image')\n",
    "    axes[2, i].text(0, 5, emotion_labels[np.argmax(label)], color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load a pre-trained model and modify the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True, progress=True)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "# Modify the classifier\n",
    "model.fc = nn.Linear(in_features, num_classes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Define the loss function, optimizer, and training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 20\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        labels = data['emotions'].argmax(dim=1).to(device)  # Modify this line\n",
    "        images = data['image'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'pretrained_resnet18_2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0366 | Accuracy: 75.24%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "val_loss = 0.0\n",
    "val_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        labels = data['emotions'].to(device)\n",
    "        images = data['image'].to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        \n",
    "        val_loss += criterion(outputs, labels).item() \n",
    "        val_correct += torch.eq(predicted, labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "val_loss /= total_samples\n",
    "val_accuracy = val_correct / total_samples\n",
    "\n",
    "print(f'Validation Loss: {val_loss:.4f} | Accuracy: {val_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for classess names in FERplus dataset\n",
    "classes = {\n",
    "    0: 'Neutral',\n",
    "    1: 'Happinnes',\n",
    "    2: 'Surprise',\n",
    "    3: 'Sadness',\n",
    "    4: 'Anger',\n",
    "    5: 'Disgust',\n",
    "    6: 'Fear',\n",
    "    7: 'Contempt',\n",
    "    8: 'Unknown',\n",
    "    9: 'NF'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 200\n",
    "\n",
    "np_img = test_dataset[idx]['image']\n",
    "plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print (f\"the original label was {classes[test_dataset[idx]['emotions'].argmax(0).item()]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=10).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 74 %\n",
      "correct: 2663, total: 3572\n",
      "[tensor(0.8125, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.5938, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.9375, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.8750, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.5625, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.8125, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.6562, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6250, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.8438, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7812, device='mps:0'), tensor(0.7500, device='mps:0'), tensor(0.6875, device='mps:0'), tensor(0.7188, device='mps:0'), tensor(0.7500, device='mps:0')]\n"
     ]
    }
   ],
   "source": [
    "# test model on the test data set\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def test_model(model, test_loader, dataset):\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_accuracy = []\n",
    "    wrong = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            labels = data['emotions']\n",
    "            inputs = data['image']\n",
    "            output = model(inputs)\n",
    "            # print(\"out: \", output.shape)\n",
    "            # print(\"labels: \", labels.shape)\n",
    "            # print(labels.shape)\n",
    "            # print(f\"test loader length:  {len(test_loader)}\")\n",
    "            # show the first 10 images and their truth labels and their predictions\n",
    "            if i < len(test_loader):\n",
    "                for j in range(len(inputs)):\n",
    "                    # if classes[labels[j].argmax(0).item()] != classes[output[j].argmax(0).item()] :\n",
    "                    #     wrong +=1\n",
    "                    \n",
    "                    \n",
    "                    file_name = \"mymodel22.txt\"\n",
    "                    output_file = open(file_name, \"a\")\n",
    "                    prediction = classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]\n",
    "                    \n",
    "                    output_file.write(f\"image: {i * batch_size + j} was {prediction}\\n\")\n",
    "                    output_file.write(f\"predicted label was: {classes[output[j].argmax(0).item()]} and the original label was {classes[labels[j].argmax(0).item()]}\\n\")\n",
    "                    output_file.write(\"=\"*50)\n",
    "                    output_file.write(\"\\n\")\n",
    "                    # if j == 10:\n",
    "                    #     print(\"one example:\")\n",
    "                    #     np_img = test_dataset[i * batch_size + j]['image']\n",
    "                    #     plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "                    #     plt.show()\n",
    "                    #     plt.close()\n",
    "                    #     print('-'*50)\n",
    "                        \n",
    "                    # print(labels[i].argmax().item())\n",
    "                    # if classes[labels[j].argmax(0).item()] == classes[output[j].argmax(0).item()]:\n",
    "                    #     print(\"hoppppaaaaaaaaaa\")\n",
    "                    #     print('image:', i * batch_size + j)\n",
    "                        \n",
    "                    #     print('truth:', classes[labels[j].argmax(0).item()])\n",
    "                    #     print('prediction:', classes[output[j].argmax(0).item()])\n",
    "                    #     print('='*50)\n",
    "                    # plt.close('all')    \n",
    "            output_file.close()\n",
    "            # measure accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            # torchmetrics_accuracy(predicted, labels)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_accuracy.append(torchmetrics_accuracy(predicted, labels))\n",
    "        # print total accuracy\n",
    "        print('Accuracy of the network on the test images: %d %%' % (\n",
    "            100 * correct / total))\n",
    "        print(f\"correct: {correct}, total: {total}\")\n",
    "        print(test_accuracy)\n",
    "test_model(model, test_loader, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./pretrained_resnet18.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0391 | Accuracy: 74.55%\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        labels = data['emotions'].to(device)\n",
    "        images = data['image'].to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        \n",
    "        test_loss += criterion(outputs, labels).item() \n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "test_loss /= total_samples\n",
    "test_accuracy = test_correct / total_samples\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f} | Accuracy: {test_accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
