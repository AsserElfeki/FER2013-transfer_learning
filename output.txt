Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 0.001215766545905694
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x10645a040> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 8.932496370954183
Max: 18.17177822608563
Average: 11.375383556169467

--------------------------------------------------
Training Accuracy: 
Min: 9.35672514619883
Max: 56.014287215043595
Average: 44.18776482123472

--------------------------------------------------
Validation Loss: 
Min: 12.34536440883364
Max: 16.723965985434397
Average: 13.017524717961042

--------------------------------------------------
Validation Accuracy: 
Min: 18.949133594186698
Max: 40.94466182224706
Average: 35.848239239798765

Execution time: 00:06:39

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 0.001215766545905694
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x106470820> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 2.253290594358327
Max: 4.558882431380559
Average: 3.0161206834460423

--------------------------------------------------
Training Accuracy: 
Min: 0.8859474034387366
Max: 36.131246279371084
Average: 27.313443288860874

--------------------------------------------------
Validation Loss: 
Min: 2.8216694934027537
Max: 4.864427785788264
Average: 3.4336802602346452

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 32.81162660704304
Average: 26.34432643935159

Execution time: 00:05:55

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 0.001215766545905694
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x29e79c340> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.267900381483397
Max: 18.38692520286842
Average: 18.287579382814187

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 7.161116363763701
Average: 1.9975837798088036

--------------------------------------------------
Validation Loss: 
Min: 18.182035497256688
Max: 18.41868370771408
Average: 18.224930715135166

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 2.5125768585802124

Execution time: 00:06:00

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 0.001215766545905694
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x2a5ba1550> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.457920104899305
Max: 4.4853188332090985
Average: 4.460325889918465

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 0.7353713625380818
Average: 0.05830444374409077

--------------------------------------------------
Validation Loss: 
Min: 4.8445377222129276
Max: 4.860286480614117
Average: 4.848971180617809

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:05:58

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 0.001215766545905694
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x158839190> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 13.877088559568262
Max: 17.631715796962965
Average: 14.938757665426973

--------------------------------------------------
Training Accuracy: 
Min: 14.794971460587597
Max: 32.629477886332594
Average: 28.185733795566755

--------------------------------------------------
Validation Loss: 
Min: 14.454872020653315
Max: 16.77495206253869
Average: 15.034399553707669

--------------------------------------------------
Validation Accuracy: 
Min: 19.200670765790946
Max: 29.70933482392398
Average: 26.788708775852427

Execution time: 00:06:53

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 0.001215766545905694
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x158831f70> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 2.136138158309099
Max: 4.081822265569467
Average: 2.647647119079031

--------------------------------------------------
Training Accuracy: 
Min: 14.567356515040096
Max: 37.91714815982071
Average: 32.379976888328606

--------------------------------------------------
Validation Loss: 
Min: 2.9641429356166293
Max: 3.9254198500088284
Average: 3.1530396789312367

--------------------------------------------------
Validation Accuracy: 
Min: 25.15371716042482
Max: 32.81162660704304
Average: 30.32560089435439

Execution time: 00:06:49

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 0.001215766545905694
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x158831f40> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.27320085207265
Max: 18.365174372545948
Average: 18.29547944506766

--------------------------------------------------
Training Accuracy: 
Min: 0.18559372483103967
Max: 6.411737927653465
Average: 2.6252757642609517

--------------------------------------------------
Validation Loss: 
Min: 18.176175781658717
Max: 18.361237032072886
Average: 18.228492905412402

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 3.7688652878703186

Execution time: 00:06:59

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 0.001215766545905694
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x10645a250> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.459459248475505
Max: 4.490466536493077
Average: 4.466775821651616

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 3.74339041215814
Average: 0.9465279966383026

--------------------------------------------------
Validation Loss: 
Min: 4.845936179161072
Max: 4.88821624645165
Average: 4.856452134890216

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 1.2562884292901062

Execution time: 00:06:54

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x158831f40> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 8.768033776811897
Max: 18.426714773274217
Average: 11.667800239913275

--------------------------------------------------
Training Accuracy: 
Min: 5.6168365024337294
Max: 55.96526245754106
Average: 42.499737367370514

--------------------------------------------------
Validation Loss: 
Min: 12.358762396233422
Max: 18.19456832749503
Average: 13.860589972351278

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 40.21799888205702
Average: 33.15399664617105

Execution time: 00:06:28

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x29e79c340> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 1.521504664741465
Max: 4.549815888634455
Average: 2.7152457510804275

--------------------------------------------------
Training Accuracy: 
Min: 2.031025667962321
Max: 44.99422208215149
Average: 30.834296319641417

--------------------------------------------------
Validation Loss: 
Min: 2.562749479498182
Max: 4.964496320911816
Average: 3.2884492636259113

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 39.46338736724427
Average: 29.78898826159866

Execution time: 00:06:15

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b293ca90> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.27139590008128
Max: 18.38667424341866
Average: 18.292094373008837

--------------------------------------------------
Training Accuracy: 
Min: 0.4762405014532339
Max: 6.534299821409812
Average: 2.6539902650838676

--------------------------------------------------
Validation Loss: 
Min: 18.18507214954921
Max: 18.47117679459708
Average: 18.221551197341512

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 1.2562884292901062

Execution time: 00:06:26

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x132a249d0> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.457348488212833
Max: 4.487957027698017
Average: 4.461396446340162

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 0.4972511118114648
Average: 0.07441257835206778

--------------------------------------------------
Validation Loss: 
Min: 4.845968380570412
Max: 4.8675946082387656
Average: 4.851971722607101

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:06:37

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2a9506130> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 14.423392220013353
Max: 17.661388119551262
Average: 15.057803317387188

--------------------------------------------------
Training Accuracy: 
Min: 11.89900899954477
Max: 30.384844346394928
Average: 27.304338691038975

--------------------------------------------------
Validation Loss: 
Min: 14.747579838548388
Max: 16.4451681801251
Average: 15.195859336853028

--------------------------------------------------
Validation Accuracy: 
Min: 19.619899385131358
Max: 28.507546115148127
Average: 25.88457238680827

Execution time: 00:06:13

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x301006b80> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 2.562797262580504
Max: 4.3061089510346315
Average: 2.9437932538492277

--------------------------------------------------
Training Accuracy: 
Min: 11.650383443639038
Max: 34.08971530622964
Average: 29.791469692194557

--------------------------------------------------
Validation Loss: 
Min: 3.2385693213769366
Max: 4.058550489800317
Average: 3.4188139817544387

--------------------------------------------------
Validation Accuracy: 
Min: 19.591950810508663
Max: 30.715483510340974
Average: 27.68166573504751

Execution time: 00:06:18

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2fdb032e0> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.26927332007738
Max: 18.361993357155384
Average: 18.30632280224782

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 6.3276954862205415
Average: 2.9532163742690054

--------------------------------------------------
Validation Loss: 
Min: 18.176201905523026
Max: 18.52166727610997
Average: 18.242526475020817

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 1.2562884292901062

Execution time: 00:06:28

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 20 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x30101b550> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.4595830229602225
Max: 4.493157784322074
Average: 4.476878867691335

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 4.955002276149456
Average: 2.822775501628322

--------------------------------------------------
Validation Loss: 
Min: 4.8442802003451755
Max: 4.908065118959972
Average: 4.864514420926571

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 3.7688652878703186

Execution time: 00:05:27

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 5.153775207320124e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x2fdb1bf70> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 8.687815018490474
Max: 18.381855952913256
Average: 10.204059976276598

--------------------------------------------------
Training Accuracy: 
Min: 5.98452218370277
Max: 57.46051756136849
Average: 50.05672864796724

--------------------------------------------------
Validation Loss: 
Min: 12.512134867055076
Max: 18.063036390713282
Average: 13.525046811274114

--------------------------------------------------
Validation Accuracy: 
Min: 0.7546115148127446
Max: 39.71492453884852
Average: 36.746226942425935

Execution time: 00:14:57

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 5.153775207320124e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x31425cf10> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 2.0013335880480314
Max: 4.565256488550149
Average: 2.543059710169426

--------------------------------------------------
Training Accuracy: 
Min: 1.5162657141856637
Max: 39.14977063417025
Average: 32.811990054977755

--------------------------------------------------
Validation Loss: 
Min: 2.772751907152789
Max: 5.059310857738767
Average: 3.1269547862453124

--------------------------------------------------
Validation Accuracy: 
Min: 4.415874790385691
Max: 35.29904974846283
Average: 31.167691447736157

Execution time: 00:14:54

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 5.153775207320124e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x32aaa6e20> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.2615686301288
Max: 18.393933617654866
Average: 18.27300287084377

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 6.989529712504815
Average: 0.7385929894596771

--------------------------------------------------
Validation Loss: 
Min: 18.175874607903616
Max: 18.308991159711564
Average: 18.198208951268878

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 1.0050307434320849

Execution time: 00:15:03

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 5.153775207320124e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x33e899fd0> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.456415386509655
Max: 4.487647870086208
Average: 4.458843667387027

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 0.5427741009209651
Average: 0.011345729593444692

--------------------------------------------------
Validation Loss: 
Min: 4.844773846013205
Max: 4.859683373144695
Average: 4.849849850748267

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:14:53

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 5.153775207320124e-05
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x3528e1fa0> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.26254199199954
Max: 18.37982230384032
Average: 18.269659033643972

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 3.137584480162482
Average: 0.13593864901775396

--------------------------------------------------
Validation Loss: 
Min: 18.178777865001134
Max: 18.25927143437522
Average: 18.195636244671686

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:16:33

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 5.153775207320124e-05
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x31ddc0e50> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.456932535897711
Max: 4.4705352452140374
Average: 4.459202206137467

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 2.3006618342262843
Average: 0.10407255664110375

--------------------------------------------------
Validation Loss: 
Min: 4.847311515893255
Max: 4.8685068509408405
Average: 4.850593769976071

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:16:13

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 5.153775207320124e-05
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x31dde5220> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.262832324422085
Max: 18.37522240582132
Average: 18.279884507434502

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 6.625345799628812
Average: 0.9863080855832195

--------------------------------------------------
Validation Loss: 
Min: 18.176449469157628
Max: 18.530000746250153
Average: 18.20856426528522

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 2.5125768585802124

Execution time: 00:16:36

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 5.153775207320124e-05
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x3ad7260d0> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.457218562368583
Max: 4.499583085200021
Average: 4.463403351475117

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 4.629337815596877
Average: 0.6025843050740624

--------------------------------------------------
Validation Loss: 
Min: 4.845825980816569
Max: 4.905327635151999
Average: 4.854595220131534

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 1.5075461151481275

Execution time: 00:16:19

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2fdb1bf70> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 7.924587958336945
Max: 18.432643086218754
Average: 9.444376810870535

--------------------------------------------------
Training Accuracy: 
Min: 5.329691494204574
Max: 60.31796057008789
Average: 52.97370171936829

--------------------------------------------------
Validation Loss: 
Min: 12.202982489551816
Max: 18.261926923479354
Average: 13.495212675162723

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 41.2800447177194
Average: 38.17887087758524

Execution time: 00:15:09

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x3528b0a30> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 0.4223790206307126
Max: 4.637859603725916
Average: 1.5028442646347795

--------------------------------------------------
Training Accuracy: 
Min: 0.010505305179115453
Max: 57.411492803865954
Average: 44.55194873411072

--------------------------------------------------
Validation Loss: 
Min: 2.367756132568632
Max: 4.847184934786388
Average: 2.7918772370368243

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 41.44773616545556
Average: 34.79094466182224

Execution time: 00:14:48

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x3f26d8e80> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.259796404224495
Max: 18.394699678709365
Average: 18.274745880780536

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 7.367720698952971
Average: 1.1420667437055712

--------------------------------------------------
Validation Loss: 
Min: 18.1748948097229
Max: 18.326672230448043
Average: 18.200384653466084

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 0.5025153717160424

Execution time: 00:15:23

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 1e-05
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x39730cc40> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.456758963701306
Max: 4.484968672407434
Average: 4.459151411184671

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 1.2466295479217004
Average: 0.12158139860629616

--------------------------------------------------
Validation Loss: 
Min: 4.843402087688446
Max: 4.861406175153596
Average: 4.848935656888145

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:14:54

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2fdb1f3a0> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 14.757107313910097
Max: 17.844606192354927
Average: 15.25880065142641

--------------------------------------------------
Training Accuracy: 
Min: 10.431768042861645
Max: 28.781034422383303
Average: 26.73859298945969

--------------------------------------------------
Validation Loss: 
Min: 14.934602328709193
Max: 16.849040959562576
Average: 15.222801247153965

--------------------------------------------------
Validation Accuracy: 
Min: 13.555058692006707
Max: 28.032420346562326
Average: 26.06092789267747

Execution time: 00:16:23

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x406dd9f70> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.4565701257742
Max: 4.5293928920088
Average: 4.4606382879457955

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 2.4442343383408622
Average: 0.10183142486955912

--------------------------------------------------
Validation Loss: 
Min: 4.84397953748703
Max: 4.881018766335079
Average: 4.850666999220848

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:16:32

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x3c5e35dc0> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.262621620067158
Max: 18.37931437134609
Average: 18.286078198231078

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 5.725391322617922
Average: 1.468501593304619

--------------------------------------------------
Validation Loss: 
Min: 18.172857778412954
Max: 18.30375795704978
Average: 18.207305815560474

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:16:24

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 50 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x406d963a0> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.457833548275647
Max: 4.49320689507855
Average: 4.470186048771473

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 4.643344889169031
Average: 1.8147564520082644

--------------------------------------------------
Validation Loss: 
Min: 4.8439721422536035
Max: 4.938558399677277
Average: 4.8627025281957215

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 2.0100614868641697

Execution time: 00:16:14

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 100 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 2.6561398887587566e-07
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x2fdb1f3a0> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 8.547495688729965
Max: 18.432060458491925
Average: 9.322234023773397

--------------------------------------------------
Training Accuracy: 
Min: 4.653850194348146
Max: 57.891235073712224
Average: 54.06541303358198

--------------------------------------------------
Validation Loss: 
Min: 12.287778317928314
Max: 18.20995988164629
Average: 13.395140534426481

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 41.22414756847401
Average: 38.36053661263281

Execution time: 00:30:47

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 100 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 2.6561398887587566e-07
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x4904a4f70> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 1.9549705145206857
Max: 4.552233218879742
Average: 2.2092021446372203

--------------------------------------------------
Training Accuracy: 
Min: 2.2621423819028608
Max: 39.59799698847918
Average: 36.647126799033515

--------------------------------------------------
Validation Loss: 
Min: 2.7737985647150447
Max: 4.84637314932687
Average: 2.931126142037765

--------------------------------------------------
Validation Accuracy: 
Min: 16.238121855785355
Max: 34.65623253214086
Average: 32.833147009502504

Execution time: 00:30:28

Finished Training with this combination
######################################################################

helloooooooo 

Parameter Combination: 
epochs: 100 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 2.6561398887587566e-07
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x4802a96a0> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.26108511191066
Max: 18.390614542688795
Average: 18.268411498342093

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 7.322197709843471
Average: 0.35472913821479857

--------------------------------------------------
Validation Loss: 
Min: 18.174706356866018
Max: 18.407796910830907
Average: 18.194079697472706

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:49:16

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 100 
learning_rate: 0.01 
batch_size: 32 
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 2.6561398887587566e-07
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x4904cdac0> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 4.456706734280303
Max: 4.490708559384405
Average: 4.458199290246739

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 0.40270336519942573
Average: 0.004307175123437337

--------------------------------------------------
Validation Loss: 
Min: 4.8453854982342035
Max: 4.8548456600734164
Average: 4.848942594804934

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:32:33

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 100 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 2.6561398887587566e-07
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x39730cc40> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.240987231990392
Max: 18.36148953090723
Average: 18.26613601295838

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 3.498266624645446
Average: 0.10897503239135763

--------------------------------------------------
Validation Loss: 
Min: 18.167990565299988
Max: 18.223615518638066
Average: 18.19045438204493

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 0.0
Average: 0.0

Execution time: 00:56:23

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 100 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 2.6561398887587566e-07
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x4f8357fd0> 
criterion: MSELoss() 

--------------------------------------------------
Training Loss: 
Min: 2.405776929615059
Max: 4.255113097614831
Average: 2.565489776946797

--------------------------------------------------
Training Accuracy: 
Min: 13.292712819974087
Max: 34.383863851244875
Average: 32.80810309206148

--------------------------------------------------
Validation Loss: 
Min: 3.3863955651010786
Max: 4.3747982404061725
Average: 3.454016238025257

--------------------------------------------------
Validation Accuracy: 
Min: 15.120178870877584
Max: 29.765231973169367
Average: 28.712688652878754

Execution time: 00:34:27

Finished Training with this combination
######################################################################
Parameter Combination: 
epochs: 100 
learning_rate: 0.01 
batch_size: 32 
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.01
    lr: 2.6561398887587566e-07
    maximize: False
    weight_decay: 0
) 
scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x4904c2d00> 
criterion: CrossEntropyLoss() 

--------------------------------------------------
Training Loss: 
Min: 18.262117365573314
Max: 18.367634026761284
Average: 18.27052931078603

--------------------------------------------------
Training Accuracy: 
Min: 0.0
Max: 5.462758693140036
Average: 0.451868193437686

--------------------------------------------------
Validation Loss: 
Min: 18.17483377456665
Max: 18.310614551816666
Average: 18.194877062780513

--------------------------------------------------
Validation Accuracy: 
Min: 0.0
Max: 25.125768585802124
Average: 0.5025153717160424

Execution time: 00:37:21

Finished Training with this combination
######################################################################
